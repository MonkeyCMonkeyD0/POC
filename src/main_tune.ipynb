{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a1ac32",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setting-up-imports\" data-toc-modified-id=\"Setting-up-imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setting up imports</a></span></li><li><span><a href=\"#Setting-up-Constant-Hyperparameters\" data-toc-modified-id=\"Setting-up-Constant-Hyperparameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setting up Constant Hyperparameters</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-the-training-function\" data-toc-modified-id=\"Creating-the-training-function-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Creating the training function</a></span></li><li><span><a href=\"#Creating-the-evaluation-function\" data-toc-modified-id=\"Creating-the-evaluation-function-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Creating the evaluation function</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Setting-up-the-loss-function-sampler\" data-toc-modified-id=\"Setting-up-the-loss-function-sampler-2.2.0.1\"><span class=\"toc-item-num\">2.2.0.1&nbsp;&nbsp;</span>Setting up the loss function sampler</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Running-the-training\" data-toc-modified-id=\"Running-the-training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Running the training</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameters-Search-Space\" data-toc-modified-id=\"Hyperparameters-Search-Space-3.0.0.1\"><span class=\"toc-item-num\">3.0.0.1&nbsp;&nbsp;</span>Hyperparameters Search Space</a></span></li><li><span><a href=\"#Selecting-Cuda-device\" data-toc-modified-id=\"Selecting-Cuda-device-3.0.0.2\"><span class=\"toc-item-num\">3.0.0.2&nbsp;&nbsp;</span>Selecting Cuda device</a></span></li><li><span><a href=\"#Loading-data-for-training\" data-toc-modified-id=\"Loading-data-for-training-3.0.0.3\"><span class=\"toc-item-num\">3.0.0.3&nbsp;&nbsp;</span>Loading data for training</a></span></li><li><span><a href=\"#Configuring-the-Tuner-with-a-Scheduler-and-a-Search-Algorithm\" data-toc-modified-id=\"Configuring-the-Tuner-with-a-Scheduler-and-a-Search-Algorithm-3.0.0.4\"><span class=\"toc-item-num\">3.0.0.4&nbsp;&nbsp;</span>Configuring the Tuner with a Scheduler and a Search Algorithm</a></span></li><li><span><a href=\"#Running-the-Tuner\" data-toc-modified-id=\"Running-the-Tuner-3.0.0.5\"><span class=\"toc-item-num\">3.0.0.5&nbsp;&nbsp;</span>Running the Tuner</a></span></li><li><span><a href=\"#Displaying-and-Evaluating-the-best-Result\" data-toc-modified-id=\"Displaying-and-Evaluating-the-best-Result-3.0.0.6\"><span class=\"toc-item-num\">3.0.0.6&nbsp;&nbsp;</span>Displaying and Evaluating the best Result</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035da606",
   "metadata": {},
   "source": [
    "# Setting up imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af16e0d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T01:44:04.754954Z",
     "start_time": "2023-03-13T01:44:02.353996Z"
    },
    "cell_style": "center",
    "init_cell": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.functional import normalize\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import GaussianBlur\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "\n",
    "from Dataset import POCDataReader, data_augment_, POCDataset\n",
    "from metrics import Metrics, EvaluationMetrics\n",
    "from models import UNet\n",
    "from loss import *\n",
    "from train import training_loop, validation_loop\n",
    "from train_tqdm import evaluation_loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9951dd",
   "metadata": {},
   "source": [
    "# Setting up Constant Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb97ae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T01:44:04.759867Z",
     "start_time": "2023-03-13T01:44:04.757058Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 12\n",
    "NUM_SAMPLES = 30\n",
    "NUM_MODEL_TEST = 5\n",
    "\n",
    "NB_AUGMENT = 2\n",
    "\n",
    "LOAD_DATA_ON_GPU = False\n",
    "GPUS_PER_TRIAL = 1\n",
    "CPUS_PER_TRIAL = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc521e6",
   "metadata": {},
   "source": [
    "## Creating the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626acb32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T01:44:04.770712Z",
     "start_time": "2023-03-13T01:44:04.761540Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def train(config, train_data, val_data):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    train_dataset = POCDataset(\n",
    "        train_data,\n",
    "        transform=normalize,\n",
    "        target_transform= GaussianBlur(kernel_size=3, sigma=0.7) if config[\"SL\"] else None,\n",
    "        negative_mining=config[\"NM\"])\n",
    "    training_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        sampler=train_dataset.sampler,\n",
    "        num_workers=CPUS_PER_TRIAL//2,\n",
    "        pin_memory=True,\n",
    "        pin_memory_device=device)\n",
    "\n",
    "    val_dataset = POCDataset(val_data, transform=normalize, target_transform=None, negative_mining=False)\n",
    "    validation_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        num_workers=CPUS_PER_TRIAL//2,\n",
    "        pin_memory=True,\n",
    "        pin_memory_device=device)\n",
    "\n",
    "    model = UNet(n_channels=1, n_classes=2, bilinear=True, crop=False)\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = config[\"loss_fn\"].to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=config[\"lr\"], betas=(0.9, 0.99))\n",
    "    lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS//2)\n",
    "\n",
    "    # To restore a checkpoint, use `session.get_checkpoint()`.\n",
    "    loaded_checkpoint = session.get_checkpoint()\n",
    "    if loaded_checkpoint:\n",
    "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "            model_state, optimizer_state, scheduler_state = torch.load(os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\"))\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "        lr_scheduler.load_state_dict(scheduler_state)\n",
    "\n",
    "    train_metrics = Metrics(\n",
    "        buffer_size=len(training_dataloader),\n",
    "        mode=\"Training\",\n",
    "        model_name=model.__class__.__name__,\n",
    "        loss_name=loss_fn.__class__.__name__,\n",
    "        opt_name=optimizer.__class__.__name__,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        learning_rate=config[\"lr\"],\n",
    "        negative_mining=config[\"NM\"],\n",
    "        soft_labels=config[\"SL\"],\n",
    "        device=device)\n",
    "\n",
    "    val_metrics = Metrics(\n",
    "        buffer_size=len(validation_dataloader),\n",
    "        mode=\"Validation\",\n",
    "        model_name=model.__class__.__name__,\n",
    "        loss_name=loss_fn.__class__.__name__,\n",
    "        opt_name=optimizer.__class__.__name__,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        learning_rate=config[\"lr\"],\n",
    "        negative_mining=config[\"NM\"],\n",
    "        soft_labels=config[\"SL\"],\n",
    "        device=device)\n",
    "\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):  # loop over the dataset multiple times\n",
    "        training_loop(epoch, training_dataloader, model, loss_fn, optimizer, lr_scheduler, train_metrics, device)\n",
    "        validation_loop(epoch, validation_dataloader, model, loss_fn, val_metrics, device)\n",
    "\n",
    "        # Here we save a checkpoint. It is automatically registered with\n",
    "        # Ray Tune and can be accessed through `session.get_checkpoint()`\n",
    "        # API in future iterations.\n",
    "        os.makedirs(\"model\", exist_ok=True)\n",
    "        torch.save((model.state_dict(), optimizer.state_dict(), lr_scheduler.state_dict()), \"model/checkpoint.pt\")\n",
    "        checkpoint = Checkpoint.from_directory(\"model\")\n",
    "        session.report(metrics=val_metrics.get_metrics(epoch), checkpoint=checkpoint)\n",
    "\n",
    "    train_metrics.close_tensorboard()\n",
    "    val_metrics.close_tensorboard()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a58ec",
   "metadata": {},
   "source": [
    "## Creating the evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da4d17c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T01:44:04.780687Z",
     "start_time": "2023-03-13T01:44:04.773025Z"
    },
    "code_folding": [
     0,
     27
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def evaluate(test_data, best_result):\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    test_dataset = POCDataset(test_data, transform=normalize, target_transform=None, negative_mining=False)\n",
    "    evaluation_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=20, pin_memory=True, pin_memory_device=device)\n",
    "\n",
    "    best_trained_model = UNet(n_channels=1, n_classes=2, bilinear=True, crop=False).to(device)\n",
    "\n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "    model_state, _, _ = torch.load(checkpoint_path)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_metrics = EvaluationMetrics(\n",
    "        buffer_size=len(evaluation_dataloader),\n",
    "        model_name=best_trained_model.__class__.__name__,\n",
    "        loss_name=best_result.config[\"loss_fn\"].__class__.__name__,\n",
    "        opt_name=\"Adam\",\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=best_result.config[\"batch_size\"],\n",
    "        learning_rate=best_result.config[\"lr\"],\n",
    "        negative_mining=best_result.config[\"NM\"],\n",
    "        soft_labels=best_result.config[\"SL\"],\n",
    "        device=device)\n",
    "\n",
    "    evaluation_loop(dataloader=evaluation_dataloader, model=best_trained_model, metric=test_metrics, device=device)\n",
    "    \n",
    "def evaluate_df(test_data, results_df):\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    test_dataset = POCDataset(test_data, transform=normalize, target_transform=None, negative_mining=False)\n",
    "    evaluation_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=20, pin_memory=True, pin_memory_device=device)\n",
    "\n",
    "    results_df.sort_values(\"CrackIoU\", ascending=False, inplace=True)\n",
    "    for index, res in results_df.head(NUM_MODEL_TEST).iterrows():\n",
    "\n",
    "        trained_model = UNet(n_channels=1, n_classes=2, bilinear=True, crop=False).to(device)\n",
    "\n",
    "        checkpoint_path = os.path.join(res[\"logdir\"], \"model/checkpoint.pt\")\n",
    "        model_state, _, _ = torch.load(checkpoint_path)\n",
    "        trained_model.load_state_dict(model_state)\n",
    "\n",
    "        test_metrics = EvaluationMetrics(\n",
    "            buffer_size=len(evaluation_dataloader),\n",
    "            model_name=trained_model.__class__.__name__,\n",
    "            loss_name=res[\"config/loss_fn\"],\n",
    "            opt_name=\"Adam\",\n",
    "            epochs=res[\"Epoch\"],\n",
    "            batch_size=res[\"config/batch_size\"],\n",
    "            learning_rate=res[\"config/lr\"],\n",
    "            negative_mining=res[\"config/NM\"],\n",
    "            soft_labels=res[\"config/SL\"],\n",
    "            device=device)\n",
    "\n",
    "        evaluation_loop(dataloader=evaluation_dataloader, model=trained_model, metric=test_metrics, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580710fe",
   "metadata": {},
   "source": [
    "#### Setting up the loss function sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05b7f919",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T01:44:04.786138Z",
     "start_time": "2023-03-13T01:44:04.782338Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def loss_fn_sampler():\n",
    "    pixel_losses_list = [\n",
    "        CrossEntropyLoss(weight=torch.tensor([.3, .7])), \n",
    "        FocalLoss(weight=torch.tensor([.3, .7]), gamma=2)\n",
    "    ]\n",
    "    volume_losses_list = [\n",
    "        JaccardLoss(),\n",
    "        TverskyLoss(alpha=0.3, beta=0.7),\n",
    "        FocalTverskyLoss(alpha=0.3, beta=0.7, gamma=2)\n",
    "    ]\n",
    "    loss_combinators_list = [ CombinedLoss, BorderedLoss ]\n",
    "\n",
    "    complete_list = pixel_losses_list + volume_losses_list\n",
    "\n",
    "    for combinator in loss_combinators_list:\n",
    "        complete_list += [combinator(loss1, loss2) for loss1 in pixel_losses_list for loss2 in volume_losses_list]\n",
    "\n",
    "    return complete_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e1c03",
   "metadata": {},
   "source": [
    "# Running the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c221d",
   "metadata": {},
   "source": [
    "#### Hyperparameters Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e136d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T01:44:04.791799Z",
     "start_time": "2023-03-13T01:44:04.787842Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"lr\": tune.qloguniform(1e-5, 1e-2, 5e-6),\n",
    "    \"batch_size\": tune.qrandint(2, 8, 2),\n",
    "    \"NM\": tune.choice([True, False]),\n",
    "    \"SL\": tune.choice([True, False]),\n",
    "    \"loss_fn\": tune.choice(loss_fn_sampler()),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48f809",
   "metadata": {},
   "source": [
    "#### Selecting Cuda device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd6be70b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T01:44:04.874948Z",
     "start_time": "2023-03-13T01:44:04.793563Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f955b19",
   "metadata": {},
   "source": [
    "#### Loading data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e15a7a2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-13T01:45:18.910151Z",
     "start_time": "2023-03-13T01:44:09.969750Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset into RAM: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2744/2744 [00:36<00:00, 75.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Loading done, RAM used: 3.78GiB / free: 117.85GiB / total: 125.40GiB\n",
      "\t- Got a total of 2744 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expending the dataset 2 more times: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 1920/1920 [00:32<00:00, 59.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Augmentation done, RAM used: 7.19GiB / free: 114.41GiB / total: 125.40GiB\n",
      "\t- Got 3840 new images and a total of 5760 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_reader = POCDataReader(root_dir=\"../data\", load_on_gpu=LOAD_DATA_ON_GPU)\n",
    "train_data, val_data, test_data = data_reader.split([0.7, 0.1, 0.2])\n",
    "data_augment_(train_data, n=NB_AUGMENT, load_on_gpu=LOAD_DATA_ON_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b0faa",
   "metadata": {},
   "source": [
    "#### Configuring the Tuner with a Scheduler and a Search Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69701ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T05:51:22.991593Z",
     "start_time": "2023-03-10T05:51:16.634452Z"
    }
   },
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(max_t=EPOCHS, grace_period=1, reduction_factor=2)\n",
    "search_algo = HyperOptSearch()\n",
    "\n",
    "tune_config = tune.TuneConfig(\n",
    "    metric=\"CrackIoU\",\n",
    "    mode=\"max\",\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    scheduler=scheduler,\n",
    "    search_alg=search_algo)\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        tune.with_parameters(train, train_data=train_data, val_data=val_data),\n",
    "        resources={\"cpu\": CPUS_PER_TRIAL, \"gpu\": GPUS_PER_TRIAL}),\n",
    "    tune_config=tune_config,\n",
    "    param_space=search_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0913de",
   "metadata": {},
   "source": [
    "#### Running the Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb3720",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-03-10T05:51:24.974Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08aabe4",
   "metadata": {},
   "source": [
    "#### Displaying and Evaluating the best Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f9ba97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T05:40:18.913144Z",
     "start_time": "2023-03-10T05:40:18.890272Z"
    }
   },
   "outputs": [],
   "source": [
    "best_result = results.get_best_result(metric=\"CrackIoU\", mode=\"max\", scope=\"all\")  # Get best result object\n",
    "print(\"Best trial config: {}\".format(best_result.config))\n",
    "print(\"Best trial final validation loss: {}\".format(best_result.metrics[\"Loss\"]))\n",
    "print(\"Best trial final validation CrackIoU: {}\".format(best_result.metrics[\"CrackIoU\"]))\n",
    "\n",
    "# evaluate(test_data=test_data, best_result=best_result)\n",
    "\n",
    "results_df = results.get_dataframe(filter_metric=\"CrackIoU\", filter_mode=\"max\")  # Get all trials by CrackIoU\n",
    "results_df.sort_values(\"CrackIoU\", ascending=False, inplace=True)\n",
    "\n",
    "evaluate_df(test_data=test_data, results_df=results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599c84a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "POC-env",
   "language": "python",
   "name": "poc-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
