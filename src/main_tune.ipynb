{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a1ac32",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setting-up-imports\" data-toc-modified-id=\"Setting-up-imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setting up imports</a></span></li><li><span><a href=\"#Setting-up-Constant-Hyperparameters\" data-toc-modified-id=\"Setting-up-Constant-Hyperparameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setting up Constant Hyperparameters</a></span></li><li><span><a href=\"#Setting-up-Parameters-and-Functions-for-Training\" data-toc-modified-id=\"Setting-up-Parameters-and-Functions-for-Training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Setting up Parameters and Functions for Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameters-Search-Space\" data-toc-modified-id=\"Hyperparameters-Search-Space-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Hyperparameters Search Space</a></span></li><li><span><a href=\"#Creating-the-training-function\" data-toc-modified-id=\"Creating-the-training-function-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Creating the training function</a></span></li><li><span><a href=\"#Creating-the-evaluation-function\" data-toc-modified-id=\"Creating-the-evaluation-function-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Creating the evaluation function</a></span></li></ul></li><li><span><a href=\"#Running-the-training\" data-toc-modified-id=\"Running-the-training-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Running the training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-data-for-training\" data-toc-modified-id=\"Loading-data-for-training-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Loading data for training</a></span></li><li><span><a href=\"#Configuring-the-Tuner-with-a-Scheduler-and-a-Search-Algorithm\" data-toc-modified-id=\"Configuring-the-Tuner-with-a-Scheduler-and-a-Search-Algorithm-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Configuring the Tuner with a Scheduler and a Search Algorithm</a></span></li><li><span><a href=\"#Running-the-Tuner\" data-toc-modified-id=\"Running-the-Tuner-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Running the Tuner</a></span></li></ul></li><li><span><a href=\"#Evaluating-the-best-Results\" data-toc-modified-id=\"Evaluating-the-best-Results-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluating the best Results</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035da606",
   "metadata": {},
   "source": [
    "# Setting up imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af16e0d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T02:44:03.810466Z",
     "start_time": "2023-04-03T02:44:01.285159Z"
    },
    "cell_style": "center",
    "init_cell": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, Sequential\n",
    "from torch.nn.functional import normalize\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import CenterCrop, Resize, GaussianBlur\n",
    "# from torchvision.transforms.functional import invert\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import session, RunConfig, CheckpointConfig\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "\n",
    "from dataset import POCDataReader, data_augment_, POCDataset\n",
    "from metrics import Metrics, EvaluationMetrics\n",
    "from models import UNet\n",
    "from loss import *\n",
    "from pipelines import InputPipeline, SequenceFilters, SumFilters\n",
    "from pipelines.filters import *\n",
    "from train import training_loop, validation_loop\n",
    "from train_tqdm import evaluation_loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9951dd",
   "metadata": {},
   "source": [
    "# Setting up Constant Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb97ae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T02:44:03.816987Z",
     "start_time": "2023-04-03T02:44:03.813625Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "NUM_SAMPLES = 300\n",
    "\n",
    "NUM_AUGMENT = 1\n",
    "\n",
    "LOAD_DATA_ON_GPU = True\n",
    "GPUS_PER_TRIAL = 1\n",
    "CPUS_PER_TRIAL = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48f809",
   "metadata": {},
   "source": [
    "##### Selecting Cuda device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd6be70b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T02:44:03.880947Z",
     "start_time": "2023-04-03T02:44:03.818994Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9687c3e3",
   "metadata": {},
   "source": [
    "# Setting up Parameters and Functions for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c221d",
   "metadata": {},
   "source": [
    "## Hyperparameters Search Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d931716",
   "metadata": {},
   "source": [
    "##### Preload Losses Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28766a0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T02:44:03.893072Z",
     "start_time": "2023-04-03T02:44:03.885443Z"
    }
   },
   "outputs": [],
   "source": [
    "crossEntropyLoss = CrossEntropyLoss(weight=torch.tensor([.3, .7]))\n",
    "focalLoss = FocalLoss(weight=torch.tensor([.3, .7]), gamma=2)\n",
    "\n",
    "jaccardLoss = JaccardLoss()\n",
    "tverskyLoss = TverskyLoss(alpha=0.3, beta=0.7)\n",
    "focalTverskyLoss = FocalTverskyLoss(alpha=0.3, beta=0.7, gamma=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939e19b",
   "metadata": {},
   "source": [
    "##### Preload Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e663e37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T02:44:03.899027Z",
     "start_time": "2023-04-03T02:44:03.895671Z"
    }
   },
   "outputs": [],
   "source": [
    "frangiFilter = FrangiFilter()\n",
    "satoFilter = SatoFilter()\n",
    "sumFilter = SumFilters(FrangiFilter(), SatoFilter())\n",
    "skeletonFilter = SequenceFilters(sumFilter, BinaryFilter(), SkeletonFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e136d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T02:44:03.904791Z",
     "start_time": "2023-04-03T02:44:03.900990Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"Network\": UNet,\n",
    "    \"Optimizer\": Adam,\n",
    "    \n",
    "    \"Learning Rate\": 1e-4,     #tune.qloguniform(1e-5, 1e-2, 5e-6),\n",
    "    \"Batch Size\": 4,           #tune.qrandint(2, 8, 2),\n",
    "\n",
    "    \"Pixel Loss\": tune.choice([crossEntropyLoss, focalLoss]),\n",
    "    \"Volume Loss\": tune.choice([jaccardLoss, tverskyLoss, focalTverskyLoss]),\n",
    "    \"Combine Loss\": tune.choice([CombinedLoss, BorderedLoss, PixelLoss, VolumeLoss]),\n",
    "    \n",
    "    \"Negative Mining\": tune.choice([True, False]),\n",
    "    \"Smooth Labeling\": tune.choice([True, False]),\n",
    "\n",
    "    \"Input Filter\": normalize,  #tune.choice([[normalize], [normalize, invert]]),\n",
    "    \"Input Layer\": tune.choice([frangiFilter, satoFilter, sumFilter, skeletonFilter]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc521e6",
   "metadata": {},
   "source": [
    "## Creating the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "626acb32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T02:44:03.920750Z",
     "start_time": "2023-04-03T02:44:03.906711Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def train(config, train_data, val_data):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    inpip = InputPipeline(\n",
    "        transformer=config[\"Input Filter\"],\n",
    "        layer_transformer=config[\"Input Layer\"])\n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        inpip = inpip.to(device)\n",
    "\n",
    "    train_dataset = POCDataset(\n",
    "        train_data,\n",
    "        transform=Sequential(inpip, CenterCrop(size=(480, 480)), Resize(size=(400, 400))),\n",
    "        target_transform= Sequential(\n",
    "            GaussianBlur(kernel_size=3, sigma=0.7),\n",
    "            CenterCrop(size=(480, 480)),\n",
    "            Resize(size=(400, 400)),\n",
    "        ) if config[\"Smooth Labeling\"] else Sequential(\n",
    "            CenterCrop(size=(480, 480)),\n",
    "            Resize(size=(400, 400)),\n",
    "        ),\n",
    "        negative_mining=config[\"Negative Mining\"],\n",
    "        load_on_gpu=LOAD_DATA_ON_GPU)\n",
    "    train_dataset.precompute_transform()\n",
    "\n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        training_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=int(config[\"Batch Size\"]),\n",
    "            sampler=train_dataset.sampler,\n",
    "            shuffle= True if train_dataset.sampler is None else None,\n",
    "        )\n",
    "    else:\n",
    "        training_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=int(config[\"Batch Size\"]),\n",
    "            sampler=train_dataset.sampler,\n",
    "            shuffle= True if train_dataset.sampler is None else None,\n",
    "            num_workers=CPUS_PER_TRIAL//2,\n",
    "            pin_memory=True,\n",
    "            pin_memory_device=device)\n",
    "\n",
    "    val_dataset = POCDataset(\n",
    "        val_data, \n",
    "        transform=Sequential(inpip, CenterCrop(size=(480, 480)), Resize(size=(400, 400))),\n",
    "        target_transform=Sequential(CenterCrop(size=(480, 480)), Resize(size=(400, 400))),\n",
    "        negative_mining=False,\n",
    "        load_on_gpu=LOAD_DATA_ON_GPU)\n",
    "    val_dataset.precompute_transform()\n",
    "    \n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=int(config[\"Batch Size\"]),\n",
    "            shuffle=True)\n",
    "    else:\n",
    "        validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=int(config[\"Batch Size\"]),\n",
    "            shuffle=True,\n",
    "            num_workers=CPUS_PER_TRIAL//2,\n",
    "            pin_memory=True,\n",
    "            pin_memory_device=device)\n",
    "\n",
    "    model = config[\"Network\"](n_channels=inpip.nb_channel, n_classes=2, bilinear=True, crop=False)\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = config[\"Combine Loss\"](config[\"Pixel Loss\"], config[\"Volume Loss\"]).to(device)\n",
    "    optimizer = config[\"Optimizer\"](model.parameters(), lr=config[\"Learning Rate\"], betas=(0.9, 0.99))\n",
    "    lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    loaded_checkpoint = session.get_checkpoint()\n",
    "    if loaded_checkpoint:\n",
    "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "            model_state, optimizer_state, scheduler_state = torch.load(os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\"))\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "        lr_scheduler.load_state_dict(scheduler_state)\n",
    "\n",
    "    train_metrics = Metrics(\n",
    "        buffer_size=len(training_dataloader),\n",
    "        mode=\"Training\",\n",
    "        hyperparam=config,\n",
    "        device=device)\n",
    "\n",
    "    val_metrics = Metrics(\n",
    "        buffer_size=len(validation_dataloader),\n",
    "        mode=\"Validation\",\n",
    "        hyperparam=config,\n",
    "        device=device)\n",
    "\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):  # loop over the dataset multiple times\n",
    "        training_loop(epoch, training_dataloader, model, loss_fn, optimizer, lr_scheduler, train_metrics, device)\n",
    "        validation_loop(epoch, validation_dataloader, model, loss_fn, val_metrics, device)\n",
    "\n",
    "        # Here we save a checkpoint. It is automatically registered with\n",
    "        # Ray Tune and can be accessed through `session.get_checkpoint()`\n",
    "        # API in future iterations.\n",
    "        os.makedirs(\"model\", exist_ok=True)\n",
    "        torch.save((model.state_dict(), optimizer.state_dict(), lr_scheduler.state_dict()), \"model/checkpoint.pt\")\n",
    "        checkpoint = Checkpoint.from_directory(\"model\")\n",
    "        session.report(metrics=val_metrics.get_metrics(epoch), checkpoint=checkpoint)\n",
    "\n",
    "    train_metrics.close_tensorboard()\n",
    "    val_metrics.close_tensorboard()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a58ec",
   "metadata": {},
   "source": [
    "## Creating the evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da4d17c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T02:44:03.928677Z",
     "start_time": "2023-04-03T02:44:03.922681Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def evaluate(test_data, result):\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    inpip = InputPipeline(\n",
    "        transformer=result.config[\"Input Filter\"], \n",
    "        layer_transformer=result.config[\"Input Layer\"])\n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        inpip = inpip.to(device)\n",
    "\n",
    "    test_dataset = POCDataset(\n",
    "        test_data,\n",
    "        transform=Sequential(inpip, CenterCrop(size=(480, 480)), Resize(size=(400, 400))),\n",
    "        target_transform=Sequential(CenterCrop(size=(480, 480)), Resize(size=(400, 400))),\n",
    "        negative_mining=False,\n",
    "        load_on_gpu=LOAD_DATA_ON_GPU)\n",
    "    \n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        evaluation_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "    else:\n",
    "        evaluation_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=20, pin_memory=True, pin_memory_device=device)\n",
    "\n",
    "    best_trained_model = result.config[\"Network\"](n_channels=inpip.nb_channel, n_classes=2, bilinear=True, crop=False).to(device)\n",
    "\n",
    "    checkpoint_path = os.path.join(result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "    model_state, _, _ = torch.load(checkpoint_path)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_metrics = EvaluationMetrics(\n",
    "        buffer_size=len(evaluation_dataloader),\n",
    "        hyperparam=result.config,\n",
    "        epochs=result.metrics[\"Epoch\"],\n",
    "        device=device)\n",
    "\n",
    "    evaluation_loop(dataloader=evaluation_dataloader, model=best_trained_model, metric=test_metrics, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e1c03",
   "metadata": {},
   "source": [
    "# Running the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f955b19",
   "metadata": {},
   "source": [
    "## Loading data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e15a7a2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T02:45:00.583517Z",
     "start_time": "2023-04-03T02:44:03.930565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e489d43cb74c67ac8ac6ca97dc80fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset into RAM:   0%|          | 0/2744 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Loading done, RAM used: 4.76GiB / free: 112.47GiB / total: 125.40GiB\n",
      "\t- Got a total of 2744 images.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300f3792a8cb4d34a3acccc63111527f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Expending the dataset 1 more times:   0%|          | 0/1920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Augmentation done, RAM used: 7.53GiB / free: 109.70GiB / total: 125.40GiB\n",
      "\t- Got 1920 new images and a total of 3840 images.\n"
     ]
    }
   ],
   "source": [
    "data_reader = POCDataReader(root_dir=\"../data/POC\", load_on_gpu=False, verbose=True)\n",
    "train_data, val_data, test_data = data_reader.split([0.7, 0.1, 0.2])\n",
    "data_augment_(train_data, n=NUM_AUGMENT, load_on_gpu=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b0faa",
   "metadata": {},
   "source": [
    "## Configuring the Tuner with a Scheduler and a Search Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a69701ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T02:45:09.094742Z",
     "start_time": "2023-04-03T02:45:00.588116Z"
    }
   },
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(max_t=EPOCHS, grace_period=2, reduction_factor=2)\n",
    "search_algo = HyperOptSearch()\n",
    "\n",
    "tune_config = tune.TuneConfig(\n",
    "    metric=\"CrackIoU\",\n",
    "    mode=\"max\",\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    scheduler=scheduler,\n",
    "    search_alg=search_algo)\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        tune.with_parameters(train, train_data=train_data, val_data=val_data),\n",
    "        resources={\"cpu\": CPUS_PER_TRIAL, \"gpu\": GPUS_PER_TRIAL}),\n",
    "    tune_config=tune_config,\n",
    "    param_space=search_space,\n",
    "    run_config=RunConfig(\n",
    "        local_dir=\"~/POC-Project/ray_results\",\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"CrackIoU\",\n",
    "            checkpoint_score_order=\"max\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0913de",
   "metadata": {},
   "source": [
    "## Running the Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb3720",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-03T02:44:01.936Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 11:45:12,903\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-04-05 16:17:57</td></tr>\n",
       "<tr><td>Running for: </td><td>2 days, 04:32:29.67</td></tr>\n",
       "<tr><td>Memory:      </td><td>46.4/125.4 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=120<br>Bracket: Iter 8.000: 0.7675126791000366 | Iter 4.000: 0.7488748729228973 | Iter 2.000: 0.7096416354179382<br>Resources requested: 40.0/40 CPUs, 2.0/2 GPUs, 0.0/65.95 GiB heap, 0.0/32.26 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  ... 103 more trials not shown (103 TERMINATED)\n",
       "  \n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name    </th><th>status    </th><th>loc                  </th><th style=\"text-align: right;\">  Batch Size</th><th>Combine Loss        </th><th>Input Filter        </th><th>Input Layer         </th><th style=\"text-align: right;\">  Learning Rate</th><th>Negative Mining  </th><th>Network             </th><th>Optimizer           </th><th>Pixel Loss        </th><th>Smooth Labeling  </th><th>Volume Loss     </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  Epoch</th><th style=\"text-align: right;\">      Loss</th><th style=\"text-align: right;\">  CrackIoU</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_5d91c32c</td><td>RUNNING   </td><td>141.223.108.122:37527</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_6200</td><td>&lt;function norma_8af0</td><td>SkeletonFilter&lt;_1ea0</td><td style=\"text-align: right;\">         0.0001</td><td>True             </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>CrossEntropyLoss()</td><td>False            </td><td>JaccardLoss     </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_72d28bcf</td><td>RUNNING   </td><td>141.223.108.122:37443</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_6200</td><td>&lt;function norma_8af0</td><td>SumFilters(Fran_1000</td><td style=\"text-align: right;\">         0.0001</td><td>False            </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>FocalLoss         </td><td>False            </td><td>FocalTverskyLoss</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_106d3641</td><td>PENDING   </td><td>                     </td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_5e40</td><td>&lt;function norma_8af0</td><td>FrangiFilter        </td><td style=\"text-align: right;\">         0.0001</td><td>True             </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>CrossEntropyLoss()</td><td>False            </td><td>JaccardLoss     </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">          </td><td style=\"text-align: right;\">          </td></tr>\n",
       "<tr><td>train_007b619b</td><td>TERMINATED</td><td>141.223.108.122:37527</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_6200</td><td>&lt;function norma_8af0</td><td>SkeletonFilter&lt;_2470</td><td style=\"text-align: right;\">         0.0001</td><td>True             </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>CrossEntropyLoss()</td><td>False            </td><td>JaccardLoss     </td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         5016.24</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.111428  </td><td style=\"text-align: right;\">  0.780831</td></tr>\n",
       "<tr><td>train_01244c56</td><td>TERMINATED</td><td>141.223.108.122:37527</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_56c0</td><td>&lt;function norma_8af0</td><td>SatoFilter          </td><td style=\"text-align: right;\">         0.0001</td><td>True             </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>CrossEntropyLoss()</td><td>False            </td><td>TverskyLoss     </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         1811.69</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.0678628 </td><td style=\"text-align: right;\">  0.731833</td></tr>\n",
       "<tr><td>train_02d111af</td><td>TERMINATED</td><td>141.223.108.122:37443</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_6200</td><td>&lt;function norma_8af0</td><td>SkeletonFilter&lt;_1ff0</td><td style=\"text-align: right;\">         0.0001</td><td>True             </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>CrossEntropyLoss()</td><td>False            </td><td>JaccardLoss     </td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         5064.09</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.109132  </td><td style=\"text-align: right;\">  0.785345</td></tr>\n",
       "<tr><td>train_08fd81bc</td><td>TERMINATED</td><td>141.223.108.122:37527</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_5a80</td><td>&lt;function norma_8af0</td><td>FrangiFilter        </td><td style=\"text-align: right;\">         0.0001</td><td>False            </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>CrossEntropyLoss()</td><td>True             </td><td>FocalTverskyLoss</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         1751.25</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.0143236 </td><td style=\"text-align: right;\">  0.693643</td></tr>\n",
       "<tr><td>train_09a5e2fa</td><td>TERMINATED</td><td>141.223.108.122:37443</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_5e40</td><td>&lt;function norma_8af0</td><td>FrangiFilter        </td><td style=\"text-align: right;\">         0.0001</td><td>True             </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>FocalLoss         </td><td>False            </td><td>JaccardLoss     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1328.22</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00126935</td><td style=\"text-align: right;\">  0.173557</td></tr>\n",
       "<tr><td>train_0ac1b7d0</td><td>TERMINATED</td><td>141.223.108.122:37443</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_5a80</td><td>&lt;function norma_8af0</td><td>SatoFilter          </td><td style=\"text-align: right;\">         0.0001</td><td>True             </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>FocalLoss         </td><td>False            </td><td>JaccardLoss     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1400.76</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0822337 </td><td style=\"text-align: right;\">  0.687803</td></tr>\n",
       "<tr><td>train_0ac2bc5d</td><td>TERMINATED</td><td>141.223.108.122:37527</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_6200</td><td>&lt;function norma_8af0</td><td>FrangiFilter        </td><td style=\"text-align: right;\">         0.0001</td><td>True             </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>CrossEntropyLoss()</td><td>False            </td><td>JaccardLoss     </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         1737.84</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.133707  </td><td style=\"text-align: right;\">  0.736862</td></tr>\n",
       "<tr><td>train_0d502892</td><td>TERMINATED</td><td>141.223.108.122:37443</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_6200</td><td>&lt;function norma_8af0</td><td>SumFilters(Fran_4400</td><td style=\"text-align: right;\">         0.0001</td><td>False            </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>CrossEntropyLoss()</td><td>False            </td><td>JaccardLoss     </td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         5023.66</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.111296  </td><td style=\"text-align: right;\">  0.781036</td></tr>\n",
       "<tr><td>train_109ba311</td><td>TERMINATED</td><td>141.223.108.122:37527</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_6200</td><td>&lt;function norma_8af0</td><td>SumFilters(Fran_c8e0</td><td style=\"text-align: right;\">         0.0001</td><td>False            </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>CrossEntropyLoss()</td><td>False            </td><td>JaccardLoss     </td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         5037.04</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.110396  </td><td style=\"text-align: right;\">  0.782813</td></tr>\n",
       "<tr><td>train_10c88a5f</td><td>TERMINATED</td><td>141.223.108.122:37527</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_6200</td><td>&lt;function norma_8af0</td><td>FrangiFilter        </td><td style=\"text-align: right;\">         0.0001</td><td>False            </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>FocalLoss         </td><td>False            </td><td>JaccardLoss     </td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         4058.43</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.111793  </td><td style=\"text-align: right;\">  0.780059</td></tr>\n",
       "<tr><td>train_11199163</td><td>TERMINATED</td><td>141.223.108.122:37443</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_5e40</td><td>&lt;function norma_8af0</td><td>SumFilters(Fran_77c0</td><td style=\"text-align: right;\">         0.0001</td><td>True             </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>FocalLoss         </td><td>False            </td><td>JaccardLoss     </td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         5004.13</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.00085689</td><td style=\"text-align: right;\">  0.326005</td></tr>\n",
       "<tr><td>train_12272893</td><td>TERMINATED</td><td>141.223.108.122:37443</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_6200</td><td>&lt;function norma_8af0</td><td>SumFilters(Fran_61d0</td><td style=\"text-align: right;\">         0.0001</td><td>False            </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>CrossEntropyLoss()</td><td>False            </td><td>JaccardLoss     </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         3546.5 </td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.127776  </td><td style=\"text-align: right;\">  0.748669</td></tr>\n",
       "<tr><td>train_139526a3</td><td>TERMINATED</td><td>141.223.108.122:37527</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_6200</td><td>&lt;function norma_8af0</td><td>SkeletonFilter&lt;_f400</td><td style=\"text-align: right;\">         0.0001</td><td>True             </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>FocalLoss         </td><td>False            </td><td>JaccardLoss     </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2268.22</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.15517   </td><td style=\"text-align: right;\">  0.695092</td></tr>\n",
       "<tr><td>train_164b724a</td><td>TERMINATED</td><td>141.223.108.122:37527</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_5e40</td><td>&lt;function norma_8af0</td><td>SkeletonFilter&lt;_f400</td><td style=\"text-align: right;\">         0.0001</td><td>False            </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>CrossEntropyLoss()</td><td>True             </td><td>FocalTverskyLoss</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         5117.68</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.00424276</td><td style=\"text-align: right;\">  0.644299</td></tr>\n",
       "<tr><td>train_20a2eb4c</td><td>TERMINATED</td><td>141.223.108.122:37443</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_5a80</td><td>&lt;function norma_8af0</td><td>SkeletonFilter&lt;_8580</td><td style=\"text-align: right;\">         0.0001</td><td>True             </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>CrossEntropyLoss()</td><td>False            </td><td>TverskyLoss     </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         3557.24</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.0593951 </td><td style=\"text-align: right;\">  0.737619</td></tr>\n",
       "<tr><td>train_21baba54</td><td>TERMINATED</td><td>141.223.108.122:37443</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_6200</td><td>&lt;function norma_8af0</td><td>SkeletonFilter&lt;_c730</td><td style=\"text-align: right;\">         0.0001</td><td>True             </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>CrossEntropyLoss()</td><td>False            </td><td>JaccardLoss     </td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         5035.01</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.112944  </td><td style=\"text-align: right;\">  0.777796</td></tr>\n",
       "<tr><td>train_21cd6481</td><td>TERMINATED</td><td>141.223.108.122:37527</td><td style=\"text-align: right;\">           4</td><td>&lt;class &#x27;loss.lo_6200</td><td>&lt;function norma_8af0</td><td>SkeletonFilter&lt;_4850</td><td style=\"text-align: right;\">         0.0001</td><td>True             </td><td>&lt;class &#x27;models._1a90</td><td>&lt;class &#x27;torch.o_4700</td><td>CrossEntropyLoss()</td><td>False            </td><td>JaccardLoss     </td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         5012.13</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.112834  </td><td style=\"text-align: right;\">  0.778092</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name    </th><th style=\"text-align: right;\">  CrackIoU</th><th style=\"text-align: right;\">  Epoch</th><th style=\"text-align: right;\">      Loss</th><th style=\"text-align: right;\">  MeanIoU</th><th style=\"text-align: right;\">  Tversky</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname           </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip        </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_007b619b</td><td style=\"text-align: right;\">  0.780831</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.111428  </td><td style=\"text-align: right;\"> 0.888572</td><td style=\"text-align: right;\"> 0.881751</td><td>2023-04-04_08-33-57</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5016.24</td><td style=\"text-align: right;\">           211.017</td><td style=\"text-align: right;\">       5016.24</td><td style=\"text-align: right;\"> 1680564837</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>007b619b  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_01244c56</td><td style=\"text-align: right;\">  0.731833</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.0678628 </td><td style=\"text-align: right;\"> 0.863426</td><td style=\"text-align: right;\"> 0.873301</td><td>2023-04-04_23-49-58</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             1811.69</td><td style=\"text-align: right;\">           211.447</td><td style=\"text-align: right;\">       1811.69</td><td style=\"text-align: right;\"> 1680619798</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>01244c56  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_02d111af</td><td style=\"text-align: right;\">  0.785345</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.109132  </td><td style=\"text-align: right;\"> 0.890868</td><td style=\"text-align: right;\"> 0.881863</td><td>2023-04-04_00-01-23</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             5064.09</td><td style=\"text-align: right;\">           210.545</td><td style=\"text-align: right;\">       5064.09</td><td style=\"text-align: right;\"> 1680534083</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>02d111af  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_08fd81bc</td><td style=\"text-align: right;\">  0.693643</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.0143236 </td><td style=\"text-align: right;\"> 0.843949</td><td style=\"text-align: right;\"> 0.851587</td><td>2023-04-03_16-13-21</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             1751.25</td><td style=\"text-align: right;\">           211.728</td><td style=\"text-align: right;\">       1751.25</td><td style=\"text-align: right;\"> 1680506001</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>08fd81bc  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_09a5e2fa</td><td style=\"text-align: right;\">  0.173557</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00126935</td><td style=\"text-align: right;\"> 0.559402</td><td style=\"text-align: right;\"> 0.384158</td><td>2023-04-05_14-40-02</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             1328.22</td><td style=\"text-align: right;\">           211.043</td><td style=\"text-align: right;\">       1328.22</td><td style=\"text-align: right;\"> 1680673202</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>09a5e2fa  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_0ac1b7d0</td><td style=\"text-align: right;\">  0.687803</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0822337 </td><td style=\"text-align: right;\"> 0.841309</td><td style=\"text-align: right;\"> 0.786823</td><td>2023-04-05_06-58-12</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             1400.76</td><td style=\"text-align: right;\">           211.086</td><td style=\"text-align: right;\">       1400.76</td><td style=\"text-align: right;\"> 1680645492</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>0ac1b7d0  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_0ac2bc5d</td><td style=\"text-align: right;\">  0.736862</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.133707  </td><td style=\"text-align: right;\"> 0.866293</td><td style=\"text-align: right;\"> 0.837537</td><td>2023-04-04_06-32-13</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             1737.84</td><td style=\"text-align: right;\">           210.829</td><td style=\"text-align: right;\">       1737.84</td><td style=\"text-align: right;\"> 1680557533</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>0ac2bc5d  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_0d502892</td><td style=\"text-align: right;\">  0.781036</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.111296  </td><td style=\"text-align: right;\"> 0.888704</td><td style=\"text-align: right;\"> 0.880192</td><td>2023-04-04_21-59-07</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             5023.66</td><td style=\"text-align: right;\">           210.228</td><td style=\"text-align: right;\">       5023.66</td><td style=\"text-align: right;\"> 1680613147</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>0d502892  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_109ba311</td><td style=\"text-align: right;\">  0.782813</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.110396  </td><td style=\"text-align: right;\"> 0.889604</td><td style=\"text-align: right;\"> 0.880102</td><td>2023-04-04_19-48-25</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5037.04</td><td style=\"text-align: right;\">           211.508</td><td style=\"text-align: right;\">       5037.04</td><td style=\"text-align: right;\"> 1680605305</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>109ba311  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_10c88a5f</td><td style=\"text-align: right;\">  0.780059</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.111793  </td><td style=\"text-align: right;\"> 0.888207</td><td style=\"text-align: right;\"> 0.878206</td><td>2023-04-05_02-21-35</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             4058.43</td><td style=\"text-align: right;\">           211.031</td><td style=\"text-align: right;\">       4058.43</td><td style=\"text-align: right;\"> 1680628895</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>10c88a5f  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_11199163</td><td style=\"text-align: right;\">  0.326005</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.00085689</td><td style=\"text-align: right;\"> 0.650811</td><td style=\"text-align: right;\"> 0.573881</td><td>2023-04-03_13-08-55</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             5004.13</td><td style=\"text-align: right;\">           210.673</td><td style=\"text-align: right;\">       5004.13</td><td style=\"text-align: right;\"> 1680494935</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>11199163  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_12272893</td><td style=\"text-align: right;\">  0.748669</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.127776  </td><td style=\"text-align: right;\"> 0.872224</td><td style=\"text-align: right;\"> 0.863131</td><td>2023-04-04_18-26-32</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         8</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             3546.5 </td><td style=\"text-align: right;\">           210.862</td><td style=\"text-align: right;\">       3546.5 </td><td style=\"text-align: right;\"> 1680600392</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>12272893  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_139526a3</td><td style=\"text-align: right;\">  0.695092</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.15517   </td><td style=\"text-align: right;\"> 0.84483 </td><td style=\"text-align: right;\"> 0.821902</td><td>2023-04-04_17-00-44</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2268.22</td><td style=\"text-align: right;\">           211.19 </td><td style=\"text-align: right;\">       2268.22</td><td style=\"text-align: right;\"> 1680595244</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>139526a3  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_164b724a</td><td style=\"text-align: right;\">  0.644299</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.00424276</td><td style=\"text-align: right;\"> 0.81863 </td><td style=\"text-align: right;\"> 0.814035</td><td>2023-04-03_13-10-52</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5117.68</td><td style=\"text-align: right;\">           211.529</td><td style=\"text-align: right;\">       5117.68</td><td style=\"text-align: right;\"> 1680495052</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>164b724a  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_20a2eb4c</td><td style=\"text-align: right;\">  0.737619</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.0593951 </td><td style=\"text-align: right;\"> 0.866345</td><td style=\"text-align: right;\"> 0.884141</td><td>2023-04-05_02-16-48</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         8</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             3557.24</td><td style=\"text-align: right;\">           211.103</td><td style=\"text-align: right;\">       3557.24</td><td style=\"text-align: right;\"> 1680628608</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>20a2eb4c  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_21baba54</td><td style=\"text-align: right;\">  0.777796</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.112944  </td><td style=\"text-align: right;\"> 0.887056</td><td style=\"text-align: right;\"> 0.880268</td><td>2023-04-05_14-17-54</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             5035.01</td><td style=\"text-align: right;\">           210.945</td><td style=\"text-align: right;\">       5035.01</td><td style=\"text-align: right;\"> 1680671874</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>21baba54  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_21cd6481</td><td style=\"text-align: right;\">  0.778092</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.112834  </td><td style=\"text-align: right;\"> 0.887166</td><td style=\"text-align: right;\"> 0.882922</td><td>2023-04-04_02-15-29</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5012.13</td><td style=\"text-align: right;\">           212.194</td><td style=\"text-align: right;\">       5012.13</td><td style=\"text-align: right;\"> 1680542129</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>21cd6481  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_2332031e</td><td style=\"text-align: right;\">  0.678394</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.154421  </td><td style=\"text-align: right;\"> 0.836064</td><td style=\"text-align: right;\"> 0.845579</td><td>2023-04-04_11-21-56</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2269.01</td><td style=\"text-align: right;\">           211.792</td><td style=\"text-align: right;\">       2269.01</td><td style=\"text-align: right;\"> 1680574916</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>2332031e  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_23ca6d19</td><td style=\"text-align: right;\">  0.670265</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0765387 </td><td style=\"text-align: right;\"> 0.831632</td><td style=\"text-align: right;\"> 0.849731</td><td>2023-04-03_19-50-21</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             1410.52</td><td style=\"text-align: right;\">           211.703</td><td style=\"text-align: right;\">       1410.52</td><td style=\"text-align: right;\"> 1680519021</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>23ca6d19  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_2526f9da</td><td style=\"text-align: right;\">  0.766291</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.118882  </td><td style=\"text-align: right;\"> 0.881118</td><td style=\"text-align: right;\"> 0.879751</td><td>2023-04-04_22-12-06</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         8</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             3580.95</td><td style=\"text-align: right;\">           211.204</td><td style=\"text-align: right;\">       3580.95</td><td style=\"text-align: right;\"> 1680613926</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>2526f9da  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_260d5e41</td><td style=\"text-align: right;\">  0.773512</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.115082  </td><td style=\"text-align: right;\"> 0.884918</td><td style=\"text-align: right;\"> 0.875036</td><td>2023-04-05_01-13-56</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5038.23</td><td style=\"text-align: right;\">           211.126</td><td style=\"text-align: right;\">       5038.23</td><td style=\"text-align: right;\"> 1680624836</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>260d5e41  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_27ec8551</td><td style=\"text-align: right;\">  0.17851 </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00134032</td><td style=\"text-align: right;\"> 0.561656</td><td style=\"text-align: right;\"> 0.395472</td><td>2023-04-05_04-04-03</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2292.07</td><td style=\"text-align: right;\">           210.841</td><td style=\"text-align: right;\">       2292.07</td><td style=\"text-align: right;\"> 1680635043</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>27ec8551  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_2a6afcef</td><td style=\"text-align: right;\">  0.739036</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.0694295 </td><td style=\"text-align: right;\"> 0.867016</td><td style=\"text-align: right;\"> 0.872406</td><td>2023-04-05_10-31-32</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2719.27</td><td style=\"text-align: right;\">           211.77 </td><td style=\"text-align: right;\">       2719.27</td><td style=\"text-align: right;\"> 1680658292</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>2a6afcef  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_2b7fb966</td><td style=\"text-align: right;\">  0.151095</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00148338</td><td style=\"text-align: right;\"> 0.547783</td><td style=\"text-align: right;\"> 0.339094</td><td>2023-04-03_13-31-07</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             1332.08</td><td style=\"text-align: right;\">           210.69 </td><td style=\"text-align: right;\">       1332.08</td><td style=\"text-align: right;\"> 1680496267</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>2b7fb966  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_2ec1a02b</td><td style=\"text-align: right;\">  0.763857</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.0548165 </td><td style=\"text-align: right;\"> 0.879771</td><td style=\"text-align: right;\"> 0.891851</td><td>2023-04-03_20-41-07</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5075.14</td><td style=\"text-align: right;\">           212.225</td><td style=\"text-align: right;\">       5075.14</td><td style=\"text-align: right;\"> 1680522067</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>2ec1a02b  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_2f48ee02</td><td style=\"text-align: right;\">  0.604133</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00560575</td><td style=\"text-align: right;\"> 0.798234</td><td style=\"text-align: right;\"> 0.771307</td><td>2023-04-04_06-36-28</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2299.58</td><td style=\"text-align: right;\">           211.24 </td><td style=\"text-align: right;\">       2299.58</td><td style=\"text-align: right;\"> 1680557788</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>2f48ee02  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_341b22e9</td><td style=\"text-align: right;\">  0.737045</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.115059  </td><td style=\"text-align: right;\"> 0.866058</td><td style=\"text-align: right;\"> 0.884941</td><td>2023-04-03_17-21-51</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             4109.09</td><td style=\"text-align: right;\">           212.327</td><td style=\"text-align: right;\">       4109.09</td><td style=\"text-align: right;\"> 1680510111</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>341b22e9  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_3f7e7932</td><td style=\"text-align: right;\">  0.683125</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0169485 </td><td style=\"text-align: right;\"> 0.83843 </td><td style=\"text-align: right;\"> 0.844425</td><td>2023-04-04_08-52-54</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2286.64</td><td style=\"text-align: right;\">           211.021</td><td style=\"text-align: right;\">       2286.64</td><td style=\"text-align: right;\"> 1680565974</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>3f7e7932  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_426fb1fb</td><td style=\"text-align: right;\">  0.645101</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0966896 </td><td style=\"text-align: right;\"> 0.81925 </td><td style=\"text-align: right;\"> 0.788774</td><td>2023-04-05_15-48-46</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             1406.62</td><td style=\"text-align: right;\">           211.071</td><td style=\"text-align: right;\">       1406.62</td><td style=\"text-align: right;\"> 1680677326</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>426fb1fb  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_48a7ec9e</td><td style=\"text-align: right;\">  0.743033</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.0663526 </td><td style=\"text-align: right;\"> 0.869161</td><td style=\"text-align: right;\"> 0.86933 </td><td>2023-04-05_12-40-33</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2722.2 </td><td style=\"text-align: right;\">           212.012</td><td style=\"text-align: right;\">       2722.2 </td><td style=\"text-align: right;\"> 1680666033</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>48a7ec9e  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_4b7634ef</td><td style=\"text-align: right;\">  0.764476</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.0563445 </td><td style=\"text-align: right;\"> 0.880106</td><td style=\"text-align: right;\"> 0.891922</td><td>2023-04-03_19-16-32</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             4156.82</td><td style=\"text-align: right;\">           211.71 </td><td style=\"text-align: right;\">       4156.82</td><td style=\"text-align: right;\"> 1680516992</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>4b7634ef  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_4c13c7b9</td><td style=\"text-align: right;\">  0.722896</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.0652947 </td><td style=\"text-align: right;\"> 0.858788</td><td style=\"text-align: right;\"> 0.872755</td><td>2023-04-05_15-25-19</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2716.82</td><td style=\"text-align: right;\">           211.048</td><td style=\"text-align: right;\">       2716.82</td><td style=\"text-align: right;\"> 1680675919</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>4c13c7b9  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_4ca876c4</td><td style=\"text-align: right;\">  0.737244</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.0721085 </td><td style=\"text-align: right;\"> 0.866403</td><td style=\"text-align: right;\"> 0.842088</td><td>2023-04-04_11-52-11</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             1814.91</td><td style=\"text-align: right;\">           211.161</td><td style=\"text-align: right;\">       1814.91</td><td style=\"text-align: right;\"> 1680576731</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>4ca876c4  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_511c998a</td><td style=\"text-align: right;\">  0.687511</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.012699  </td><td style=\"text-align: right;\"> 0.840604</td><td style=\"text-align: right;\"> 0.855817</td><td>2023-04-05_09-45-08</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2293.18</td><td style=\"text-align: right;\">           211.084</td><td style=\"text-align: right;\">       2293.18</td><td style=\"text-align: right;\"> 1680655508</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>511c998a  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_52067e3f</td><td style=\"text-align: right;\">  0.728649</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.137906  </td><td style=\"text-align: right;\"> 0.862094</td><td style=\"text-align: right;\"> 0.816087</td><td>2023-04-04_20-35-23</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2709.38</td><td style=\"text-align: right;\">           210.136</td><td style=\"text-align: right;\">       2709.38</td><td style=\"text-align: right;\"> 1680608123</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>52067e3f  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_5240c1dd</td><td style=\"text-align: right;\">  0.771   </td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.0598602 </td><td style=\"text-align: right;\"> 0.883516</td><td style=\"text-align: right;\"> 0.881955</td><td>2023-04-05_00-01-12</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             5036.59</td><td style=\"text-align: right;\">           211.342</td><td style=\"text-align: right;\">       5036.59</td><td style=\"text-align: right;\"> 1680620472</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>5240c1dd  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_535cfd10</td><td style=\"text-align: right;\">  0.745564</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.129342  </td><td style=\"text-align: right;\"> 0.870658</td><td style=\"text-align: right;\"> 0.84993 </td><td>2023-04-05_08-21-39</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2714.97</td><td style=\"text-align: right;\">           210.285</td><td style=\"text-align: right;\">       2714.97</td><td style=\"text-align: right;\"> 1680650499</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>535cfd10  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_542b0e7c</td><td style=\"text-align: right;\">  0.584598</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00581797</td><td style=\"text-align: right;\"> 0.787631</td><td style=\"text-align: right;\"> 0.781456</td><td>2023-04-05_06-34-51</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2288.81</td><td style=\"text-align: right;\">           210.859</td><td style=\"text-align: right;\">       2288.81</td><td style=\"text-align: right;\"> 1680644091</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>542b0e7c  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_55c9da3f</td><td style=\"text-align: right;\">  0.701166</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.152128  </td><td style=\"text-align: right;\"> 0.847872</td><td style=\"text-align: right;\"> 0.838426</td><td>2023-04-04_07-10-20</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2286.87</td><td style=\"text-align: right;\">           211.154</td><td style=\"text-align: right;\">       2286.87</td><td style=\"text-align: right;\"> 1680559820</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>55c9da3f  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_5e2459d2</td><td style=\"text-align: right;\">  0.778522</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.112568  </td><td style=\"text-align: right;\"> 0.887432</td><td style=\"text-align: right;\"> 0.88171 </td><td>2023-04-04_14-15-53</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5039.92</td><td style=\"text-align: right;\">           211.342</td><td style=\"text-align: right;\">       5039.92</td><td style=\"text-align: right;\"> 1680585353</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>5e2459d2  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_60d4d2b1</td><td style=\"text-align: right;\">  0.661426</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0137332 </td><td style=\"text-align: right;\"> 0.827026</td><td style=\"text-align: right;\"> 0.846991</td><td>2023-04-04_10-44-07</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             1394.89</td><td style=\"text-align: right;\">           212.912</td><td style=\"text-align: right;\">       1394.89</td><td style=\"text-align: right;\"> 1680572647</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>60d4d2b1  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_65c06e5d</td><td style=\"text-align: right;\">  0.724206</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.0695806 </td><td style=\"text-align: right;\"> 0.859449</td><td style=\"text-align: right;\"> 0.86958 </td><td>2023-04-04_13-33-14</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2718.69</td><td style=\"text-align: right;\">           210.978</td><td style=\"text-align: right;\">       2718.69</td><td style=\"text-align: right;\"> 1680582794</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>65c06e5d  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_66c69f36</td><td style=\"text-align: right;\">  0.778966</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.112413  </td><td style=\"text-align: right;\"> 0.887587</td><td style=\"text-align: right;\"> 0.883348</td><td>2023-04-03_22-36-59</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             5049.91</td><td style=\"text-align: right;\">           211.004</td><td style=\"text-align: right;\">       5049.91</td><td style=\"text-align: right;\"> 1680529019</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>66c69f36  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_67f23d8b</td><td style=\"text-align: right;\">  0.781019</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.1113    </td><td style=\"text-align: right;\"> 0.8887  </td><td style=\"text-align: right;\"> 0.878728</td><td>2023-04-05_07-39-37</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             4064.24</td><td style=\"text-align: right;\">           212.033</td><td style=\"text-align: right;\">       4064.24</td><td style=\"text-align: right;\"> 1680647977</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>67f23d8b  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_68866641</td><td style=\"text-align: right;\">  0.754992</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.00858503</td><td style=\"text-align: right;\"> 0.875252</td><td style=\"text-align: right;\"> 0.886185</td><td>2023-04-03_16-56-34</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             5026.39</td><td style=\"text-align: right;\">           211.331</td><td style=\"text-align: right;\">       5026.39</td><td style=\"text-align: right;\"> 1680508594</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>68866641  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_68a5db04</td><td style=\"text-align: right;\">  0.692525</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.145491  </td><td style=\"text-align: right;\"> 0.843239</td><td style=\"text-align: right;\"> 0.854509</td><td>2023-04-04_09-14-57</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             1322.56</td><td style=\"text-align: right;\">           210.411</td><td style=\"text-align: right;\">       1322.56</td><td style=\"text-align: right;\"> 1680567297</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>68a5db04  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_69864051</td><td style=\"text-align: right;\">  0.733058</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.135645  </td><td style=\"text-align: right;\"> 0.864355</td><td style=\"text-align: right;\"> 0.834486</td><td>2023-04-04_16-10-58</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2686.22</td><td style=\"text-align: right;\">           210.382</td><td style=\"text-align: right;\">       2686.22</td><td style=\"text-align: right;\"> 1680592258</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>69864051  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_69f53ba3</td><td style=\"text-align: right;\">  0.698948</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0231264 </td><td style=\"text-align: right;\"> 0.846512</td><td style=\"text-align: right;\"> 0.854346</td><td>2023-04-04_16-49-27</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2308.17</td><td style=\"text-align: right;\">           210.913</td><td style=\"text-align: right;\">       2308.17</td><td style=\"text-align: right;\"> 1680594567</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>69f53ba3  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_6b3b85be</td><td style=\"text-align: right;\">  0.579562</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00608501</td><td style=\"text-align: right;\"> 0.785563</td><td style=\"text-align: right;\"> 0.750753</td><td>2023-04-05_01-17-30</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2290.36</td><td style=\"text-align: right;\">           211.177</td><td style=\"text-align: right;\">       2290.36</td><td style=\"text-align: right;\"> 1680625050</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>6b3b85be  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_6f6b7bb3</td><td style=\"text-align: right;\">  0.733866</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.0603996 </td><td style=\"text-align: right;\"> 0.864474</td><td style=\"text-align: right;\"> 0.882006</td><td>2023-04-03_15-44-10</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5040.48</td><td style=\"text-align: right;\">           212.616</td><td style=\"text-align: right;\">       5040.48</td><td style=\"text-align: right;\"> 1680504250</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>6f6b7bb3  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_7033fdb0</td><td style=\"text-align: right;\">  0.761156</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.0647807 </td><td style=\"text-align: right;\"> 0.87846 </td><td style=\"text-align: right;\"> 0.879656</td><td>2023-04-04_16-22-56</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         8</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             3538.27</td><td style=\"text-align: right;\">           211.203</td><td style=\"text-align: right;\">       3538.27</td><td style=\"text-align: right;\"> 1680592976</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>7033fdb0  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_70e2789c</td><td style=\"text-align: right;\">  0.64373 </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0321812 </td><td style=\"text-align: right;\"> 0.818053</td><td style=\"text-align: right;\"> 0.829872</td><td>2023-04-05_00-39-20</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2287.64</td><td style=\"text-align: right;\">           210.241</td><td style=\"text-align: right;\">       2287.64</td><td style=\"text-align: right;\"> 1680622760</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>70e2789c  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_72a89f4b</td><td style=\"text-align: right;\">  0.671065</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.014976  </td><td style=\"text-align: right;\"> 0.832234</td><td style=\"text-align: right;\"> 0.842822</td><td>2023-04-03_17-45-11</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             1400.31</td><td style=\"text-align: right;\">           212.529</td><td style=\"text-align: right;\">       1400.31</td><td style=\"text-align: right;\"> 1680511511</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>72a89f4b  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_7308507b</td><td style=\"text-align: right;\">  0.754569</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.124939  </td><td style=\"text-align: right;\"> 0.875061</td><td style=\"text-align: right;\"> 0.881324</td><td>2023-04-04_04-34-01</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         8</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             3550.27</td><td style=\"text-align: right;\">           210.267</td><td style=\"text-align: right;\">       3550.27</td><td style=\"text-align: right;\"> 1680550441</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>7308507b  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_743aa1b0</td><td style=\"text-align: right;\">  0.766622</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.0608834 </td><td style=\"text-align: right;\"> 0.881245</td><td style=\"text-align: right;\"> 0.884921</td><td>2023-04-05_05-07-51</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         8</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2671.45</td><td style=\"text-align: right;\">           211.911</td><td style=\"text-align: right;\">       2671.45</td><td style=\"text-align: right;\"> 1680638871</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>743aa1b0  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_77d2b32c</td><td style=\"text-align: right;\">  0.723225</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.0173728 </td><td style=\"text-align: right;\"> 0.858957</td><td style=\"text-align: right;\"> 0.874012</td><td>2023-04-05_14-27-32</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2719.81</td><td style=\"text-align: right;\">           212.206</td><td style=\"text-align: right;\">       2719.81</td><td style=\"text-align: right;\"> 1680672452</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>77d2b32c  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_799a787e</td><td style=\"text-align: right;\">  0.700864</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0791557 </td><td style=\"text-align: right;\"> 0.847565</td><td style=\"text-align: right;\"> 0.85172 </td><td>2023-04-05_13-03-57</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             1403.52</td><td style=\"text-align: right;\">           211.791</td><td style=\"text-align: right;\">       1403.52</td><td style=\"text-align: right;\"> 1680667437</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>799a787e  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_7ba6dba9</td><td style=\"text-align: right;\">  0.776922</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.0585563 </td><td style=\"text-align: right;\"> 0.886619</td><td style=\"text-align: right;\"> 0.878224</td><td>2023-04-04_09-43-05</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             4148.67</td><td style=\"text-align: right;\">           211.961</td><td style=\"text-align: right;\">       4148.67</td><td style=\"text-align: right;\"> 1680568985</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>7ba6dba9  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_7f499e39</td><td style=\"text-align: right;\">  0.762071</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.110185  </td><td style=\"text-align: right;\"> 0.878909</td><td style=\"text-align: right;\"> 0.889815</td><td>2023-04-03_14-20-09</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             4157.22</td><td style=\"text-align: right;\">           211.555</td><td style=\"text-align: right;\">       4157.22</td><td style=\"text-align: right;\"> 1680499209</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>7f499e39  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_837bb82a</td><td style=\"text-align: right;\">  0.727024</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.011588  </td><td style=\"text-align: right;\"> 0.860956</td><td style=\"text-align: right;\"> 0.875527</td><td>2023-04-03_14-54-49</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             5022.27</td><td style=\"text-align: right;\">           211.218</td><td style=\"text-align: right;\">       5022.27</td><td style=\"text-align: right;\"> 1680501289</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>837bb82a  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_8491f641</td><td style=\"text-align: right;\">  0.683706</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.144209  </td><td style=\"text-align: right;\"> 0.838693</td><td style=\"text-align: right;\"> 0.855791</td><td>2023-04-04_12-16-58</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2305.95</td><td style=\"text-align: right;\">           211.504</td><td style=\"text-align: right;\">       2305.95</td><td style=\"text-align: right;\"> 1680578218</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>8491f641  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_8755d914</td><td style=\"text-align: right;\">  0.559531</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00656024</td><td style=\"text-align: right;\"> 0.774999</td><td style=\"text-align: right;\"> 0.743151</td><td>2023-04-04_12-14-06</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             1314.64</td><td style=\"text-align: right;\">           211.198</td><td style=\"text-align: right;\">       1314.64</td><td style=\"text-align: right;\"> 1680578046</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>8755d914  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_89d6c935</td><td style=\"text-align: right;\">  0.766774</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.11861   </td><td style=\"text-align: right;\"> 0.88139 </td><td style=\"text-align: right;\"> 0.880104</td><td>2023-04-05_12-53-59</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         8</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             3560.61</td><td style=\"text-align: right;\">           210.863</td><td style=\"text-align: right;\">       3560.61</td><td style=\"text-align: right;\"> 1680666839</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>89d6c935  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_9227d3e4</td><td style=\"text-align: right;\">  0.5652  </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00574798</td><td style=\"text-align: right;\"> 0.777914</td><td style=\"text-align: right;\"> 0.758889</td><td>2023-04-04_14-53-41</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2267.37</td><td style=\"text-align: right;\">           211.528</td><td style=\"text-align: right;\">       2267.37</td><td style=\"text-align: right;\"> 1680587621</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>9227d3e4  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_9324d9b7</td><td style=\"text-align: right;\">  0.683399</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.141032  </td><td style=\"text-align: right;\"> 0.838297</td><td style=\"text-align: right;\"> 0.858968</td><td>2023-04-05_04-33-18</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             1754.76</td><td style=\"text-align: right;\">           210.964</td><td style=\"text-align: right;\">       1754.76</td><td style=\"text-align: right;\"> 1680636798</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>9324d9b7  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_93dddfe6</td><td style=\"text-align: right;\">  0.556879</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00627275</td><td style=\"text-align: right;\"> 0.773147</td><td style=\"text-align: right;\"> 0.771873</td><td>2023-04-04_08-14-47</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             1321.93</td><td style=\"text-align: right;\">           210.167</td><td style=\"text-align: right;\">       1321.93</td><td style=\"text-align: right;\"> 1680563687</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>93dddfe6  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_94113aa9</td><td style=\"text-align: right;\">  0.779639</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.112021  </td><td style=\"text-align: right;\"> 0.887979</td><td style=\"text-align: right;\"> 0.878941</td><td>2023-04-04_23-19-46</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             4059.46</td><td style=\"text-align: right;\">           210.999</td><td style=\"text-align: right;\">       4059.46</td><td style=\"text-align: right;\"> 1680617986</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>94113aa9  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_95666f27</td><td style=\"text-align: right;\">  0.778312</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.112701  </td><td style=\"text-align: right;\"> 0.887299</td><td style=\"text-align: right;\"> 0.881811</td><td>2023-04-04_05-04-13</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5055.25</td><td style=\"text-align: right;\">           211.277</td><td style=\"text-align: right;\">       5055.25</td><td style=\"text-align: right;\"> 1680552253</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>95666f27  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_95aaf4d7</td><td style=\"text-align: right;\">  0.666391</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.169613  </td><td style=\"text-align: right;\"> 0.830387</td><td style=\"text-align: right;\"> 0.766791</td><td>2023-04-04_15-23-57</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             1816.63</td><td style=\"text-align: right;\">           210.948</td><td style=\"text-align: right;\">       1816.63</td><td style=\"text-align: right;\"> 1680589437</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>95aaf4d7  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_9a80edbe</td><td style=\"text-align: right;\">  0.706019</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0133457 </td><td style=\"text-align: right;\"> 0.850192</td><td style=\"text-align: right;\"> 0.857791</td><td>2023-04-05_05-11-30</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2291.4 </td><td style=\"text-align: right;\">           211.923</td><td style=\"text-align: right;\">       2291.4 </td><td style=\"text-align: right;\"> 1680639090</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>9a80edbe  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_9ac3a837</td><td style=\"text-align: right;\">  0.781022</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.111375  </td><td style=\"text-align: right;\"> 0.888625</td><td style=\"text-align: right;\"> 0.885945</td><td>2023-04-05_06-31-53</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5041.53</td><td style=\"text-align: right;\">           211.801</td><td style=\"text-align: right;\">       5041.53</td><td style=\"text-align: right;\"> 1680643913</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>9ac3a837  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_9ba7cd4c</td><td style=\"text-align: right;\">  0.16888 </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00135515</td><td style=\"text-align: right;\"> 0.556697</td><td style=\"text-align: right;\"> 0.375802</td><td>2023-04-03_15-32-47</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2277.83</td><td style=\"text-align: right;\">           210.177</td><td style=\"text-align: right;\">       2277.83</td><td style=\"text-align: right;\"> 1680503567</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>9ba7cd4c  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_a2378fc4</td><td style=\"text-align: right;\">  0.760581</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.121823  </td><td style=\"text-align: right;\"> 0.878177</td><td style=\"text-align: right;\"> 0.882584</td><td>2023-04-04_06-03-15</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         8</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             3541.46</td><td style=\"text-align: right;\">           211.284</td><td style=\"text-align: right;\">       3541.46</td><td style=\"text-align: right;\"> 1680555795</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>a2378fc4  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_a8605a7e</td><td style=\"text-align: right;\">  0.691841</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0759091 </td><td style=\"text-align: right;\"> 0.842861</td><td style=\"text-align: right;\"> 0.857365</td><td>2023-04-04_10-20-52</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2265.95</td><td style=\"text-align: right;\">           212.117</td><td style=\"text-align: right;\">       2265.95</td><td style=\"text-align: right;\"> 1680571252</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>a8605a7e  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_aac35a63</td><td style=\"text-align: right;\">  0.719179</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.00964754</td><td style=\"text-align: right;\"> 0.856831</td><td style=\"text-align: right;\"> 0.874766</td><td>2023-04-03_21-12-49</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         8</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             3544.48</td><td style=\"text-align: right;\">           211.386</td><td style=\"text-align: right;\">       3544.48</td><td style=\"text-align: right;\"> 1680523969</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>aac35a63  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_ab95fe12</td><td style=\"text-align: right;\">  0.780554</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.11153   </td><td style=\"text-align: right;\"> 0.88847 </td><td style=\"text-align: right;\"> 0.878732</td><td>2023-04-04_21-12-25</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5039.68</td><td style=\"text-align: right;\">           211.97 </td><td style=\"text-align: right;\">       5039.68</td><td style=\"text-align: right;\"> 1680610345</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>ab95fe12  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_ac8566fe</td><td style=\"text-align: right;\">  0.766972</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.11858   </td><td style=\"text-align: right;\"> 0.88142 </td><td style=\"text-align: right;\"> 0.882938</td><td>2023-04-05_09-46-12</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         8</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2580.24</td><td style=\"text-align: right;\">           211.164</td><td style=\"text-align: right;\">       2580.24</td><td style=\"text-align: right;\"> 1680655572</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>ac8566fe  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_b02584c8</td><td style=\"text-align: right;\">  0.776655</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.113553  </td><td style=\"text-align: right;\"> 0.886447</td><td style=\"text-align: right;\"> 0.881132</td><td>2023-04-04_00-51-56</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5011.38</td><td style=\"text-align: right;\">           211.143</td><td style=\"text-align: right;\">       5011.38</td><td style=\"text-align: right;\"> 1680537116</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>b02584c8  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_b0e58cd8</td><td style=\"text-align: right;\">  0.745493</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.129543  </td><td style=\"text-align: right;\"> 0.870457</td><td style=\"text-align: right;\"> 0.870615</td><td>2023-04-05_05-56-42</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2711.97</td><td style=\"text-align: right;\">           210.698</td><td style=\"text-align: right;\">       2711.97</td><td style=\"text-align: right;\"> 1680641802</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>b0e58cd8  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_b1416505</td><td style=\"text-align: right;\">  0.776246</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.0589859 </td><td style=\"text-align: right;\"> 0.886233</td><td style=\"text-align: right;\"> 0.87963 </td><td>2023-04-03_23-28-25</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5019.08</td><td style=\"text-align: right;\">           212.165</td><td style=\"text-align: right;\">       5019.08</td><td style=\"text-align: right;\"> 1680532105</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>b1416505  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_b206779d</td><td style=\"text-align: right;\">  0.526257</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00631827</td><td style=\"text-align: right;\"> 0.75739 </td><td style=\"text-align: right;\"> 0.736318</td><td>2023-04-05_11-55-10</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2296.42</td><td style=\"text-align: right;\">           211.456</td><td style=\"text-align: right;\">       2296.42</td><td style=\"text-align: right;\"> 1680663310</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>b206779d  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_b318bffe</td><td style=\"text-align: right;\">  0.778076</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.112789  </td><td style=\"text-align: right;\"> 0.887211</td><td style=\"text-align: right;\"> 0.879826</td><td>2023-04-04_02-10-44</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             5019.98</td><td style=\"text-align: right;\">           210.355</td><td style=\"text-align: right;\">       5019.98</td><td style=\"text-align: right;\"> 1680541844</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>b318bffe  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_b568fec6</td><td style=\"text-align: right;\">  0.778414</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.112621  </td><td style=\"text-align: right;\"> 0.887379</td><td style=\"text-align: right;\"> 0.877446</td><td>2023-04-04_14-57-03</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             5029.46</td><td style=\"text-align: right;\">           210.886</td><td style=\"text-align: right;\">       5029.46</td><td style=\"text-align: right;\"> 1680587823</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>b568fec6  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_b5e99c3f</td><td style=\"text-align: right;\">  0.784011</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.109805  </td><td style=\"text-align: right;\"> 0.890195</td><td style=\"text-align: right;\"> 0.88017 </td><td>2023-04-04_19-50-13</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             5021.12</td><td style=\"text-align: right;\">           210.614</td><td style=\"text-align: right;\">       5021.12</td><td style=\"text-align: right;\"> 1680605413</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>b5e99c3f  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_ba2044e3</td><td style=\"text-align: right;\">  0.611099</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00537573</td><td style=\"text-align: right;\"> 0.801391</td><td style=\"text-align: right;\"> 0.798122</td><td>2023-04-04_07-14-42</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2293.84</td><td style=\"text-align: right;\">           211.129</td><td style=\"text-align: right;\">       2293.84</td><td style=\"text-align: right;\"> 1680560082</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>ba2044e3  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_bbee7479</td><td style=\"text-align: right;\">  0.169266</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00137078</td><td style=\"text-align: right;\"> 0.556974</td><td style=\"text-align: right;\"> 0.377072</td><td>2023-04-03_18-07-15</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             1323.82</td><td style=\"text-align: right;\">           212.63 </td><td style=\"text-align: right;\">       1323.82</td><td style=\"text-align: right;\"> 1680512835</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>bbee7479  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_bc77ca7a</td><td style=\"text-align: right;\">  0.700648</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.152707  </td><td style=\"text-align: right;\"> 0.847293</td><td style=\"text-align: right;\"> 0.86117 </td><td>2023-04-05_13-42-12</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2295.09</td><td style=\"text-align: right;\">           211.539</td><td style=\"text-align: right;\">       2295.09</td><td style=\"text-align: right;\"> 1680669732</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>bc77ca7a  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_be3994fe</td><td style=\"text-align: right;\">  0.65753 </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0812774 </td><td style=\"text-align: right;\"> 0.825068</td><td style=\"text-align: right;\"> 0.845615</td><td>2023-04-03_19-26-51</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             1409.93</td><td style=\"text-align: right;\">           211.634</td><td style=\"text-align: right;\">       1409.93</td><td style=\"text-align: right;\"> 1680517611</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>be3994fe  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_bf1f761c</td><td style=\"text-align: right;\">  0.778009</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.112856  </td><td style=\"text-align: right;\"> 0.887144</td><td style=\"text-align: right;\"> 0.880273</td><td>2023-04-04_05-58-08</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             5047.16</td><td style=\"text-align: right;\">           210.463</td><td style=\"text-align: right;\">       5047.16</td><td style=\"text-align: right;\"> 1680555488</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>bf1f761c  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_c00f6ffe</td><td style=\"text-align: right;\">  0.712375</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.132834  </td><td style=\"text-align: right;\"> 0.853276</td><td style=\"text-align: right;\"> 0.867166</td><td>2023-04-05_11-16-54</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2721.95</td><td style=\"text-align: right;\">           211.44 </td><td style=\"text-align: right;\">       2721.95</td><td style=\"text-align: right;\"> 1680661014</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>c00f6ffe  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_c10a100e</td><td style=\"text-align: right;\">  0.683186</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0289134 </td><td style=\"text-align: right;\"> 0.838264</td><td style=\"text-align: right;\"> 0.839513</td><td>2023-04-04_12-51-53</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2267   </td><td style=\"text-align: right;\">           212.058</td><td style=\"text-align: right;\">       2267   </td><td style=\"text-align: right;\"> 1680580313</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>c10a100e  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_c33955d3</td><td style=\"text-align: right;\">  0.578706</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00579192</td><td style=\"text-align: right;\"> 0.784934</td><td style=\"text-align: right;\"> 0.764253</td><td>2023-04-04_11-00-33</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             1324.01</td><td style=\"text-align: right;\">           210.542</td><td style=\"text-align: right;\">       1324.01</td><td style=\"text-align: right;\"> 1680573633</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>c33955d3  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_c3e45856</td><td style=\"text-align: right;\">  0.655652</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.17522   </td><td style=\"text-align: right;\"> 0.82478 </td><td style=\"text-align: right;\"> 0.757333</td><td>2023-04-04_17-27-25</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2278.31</td><td style=\"text-align: right;\">           210.828</td><td style=\"text-align: right;\">       2278.31</td><td style=\"text-align: right;\"> 1680596845</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>c3e45856  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_c542076b</td><td style=\"text-align: right;\">  0.776133</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.113769  </td><td style=\"text-align: right;\"> 0.886231</td><td style=\"text-align: right;\"> 0.87801 </td><td>2023-04-04_03-34-50</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             5046.37</td><td style=\"text-align: right;\">           210.102</td><td style=\"text-align: right;\">       5046.37</td><td style=\"text-align: right;\"> 1680546890</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>c542076b  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_c6b2af3d</td><td style=\"text-align: right;\">  0.730127</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.0627486 </td><td style=\"text-align: right;\"> 0.862542</td><td style=\"text-align: right;\"> 0.87613 </td><td>2023-04-04_15-26-12</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             1748.63</td><td style=\"text-align: right;\">           212.22 </td><td style=\"text-align: right;\">       1748.63</td><td style=\"text-align: right;\"> 1680589572</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>c6b2af3d  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_c6dcf4e0</td><td style=\"text-align: right;\">  0.679328</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0137899 </td><td style=\"text-align: right;\"> 0.836342</td><td style=\"text-align: right;\"> 0.84576 </td><td>2023-04-04_11-38-32</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2279.41</td><td style=\"text-align: right;\">           212.085</td><td style=\"text-align: right;\">       2279.41</td><td style=\"text-align: right;\"> 1680575912</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>c6dcf4e0  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_cae81d20</td><td style=\"text-align: right;\">  0.780079</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.111786  </td><td style=\"text-align: right;\"> 0.888214</td><td style=\"text-align: right;\"> 0.876257</td><td>2023-04-05_03-25-51</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             4142.84</td><td style=\"text-align: right;\">           210.94 </td><td style=\"text-align: right;\">       4142.84</td><td style=\"text-align: right;\"> 1680632751</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>cae81d20  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_cc4f9552</td><td style=\"text-align: right;\">  0.688074</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0157277 </td><td style=\"text-align: right;\"> 0.840867</td><td style=\"text-align: right;\"> 0.853372</td><td>2023-04-05_07-36-23</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2291.15</td><td style=\"text-align: right;\">           210.621</td><td style=\"text-align: right;\">       2291.15</td><td style=\"text-align: right;\"> 1680647783</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>cc4f9552  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_d1412dcd</td><td style=\"text-align: right;\">  0.781476</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.111113  </td><td style=\"text-align: right;\"> 0.888887</td><td style=\"text-align: right;\"> 0.884445</td><td>2023-04-04_03-39-58</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5069.02</td><td style=\"text-align: right;\">           211.445</td><td style=\"text-align: right;\">       5069.02</td><td style=\"text-align: right;\"> 1680547198</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>d1412dcd  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_d3f6869b</td><td style=\"text-align: right;\">  0.697354</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.153939  </td><td style=\"text-align: right;\"> 0.846061</td><td style=\"text-align: right;\"> 0.814782</td><td>2023-04-05_04-23-19</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2295.41</td><td style=\"text-align: right;\">           211.657</td><td style=\"text-align: right;\">       2295.41</td><td style=\"text-align: right;\"> 1680636199</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>d3f6869b  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_d4d424b1</td><td style=\"text-align: right;\">  0.569829</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00585612</td><td style=\"text-align: right;\"> 0.78027 </td><td style=\"text-align: right;\"> 0.758238</td><td>2023-04-04_22-37-15</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2288.31</td><td style=\"text-align: right;\">           210.099</td><td style=\"text-align: right;\">       2288.31</td><td style=\"text-align: right;\"> 1680615435</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>d4d424b1  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_d86de3ec</td><td style=\"text-align: right;\">  0.773116</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.0598712 </td><td style=\"text-align: right;\"> 0.884691</td><td style=\"text-align: right;\"> 0.877863</td><td>2023-04-03_22-04-46</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5018.07</td><td style=\"text-align: right;\">           212.237</td><td style=\"text-align: right;\">       5018.07</td><td style=\"text-align: right;\"> 1680527086</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>d86de3ec  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_de188a9a</td><td style=\"text-align: right;\">  0.781746</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.11096   </td><td style=\"text-align: right;\"> 0.88904 </td><td style=\"text-align: right;\"> 0.881164</td><td>2023-04-05_10-54-15</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             4146.45</td><td style=\"text-align: right;\">           210.935</td><td style=\"text-align: right;\">       4146.45</td><td style=\"text-align: right;\"> 1680659655</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>de188a9a  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_defa9a65</td><td style=\"text-align: right;\">  0.718612</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.0726052 </td><td style=\"text-align: right;\"> 0.856564</td><td style=\"text-align: right;\"> 0.867072</td><td>2023-04-04_12-47-55</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             1856.17</td><td style=\"text-align: right;\">           211.523</td><td style=\"text-align: right;\">       1856.17</td><td style=\"text-align: right;\"> 1680580075</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>defa9a65  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_e0a0fca2</td><td style=\"text-align: right;\">  0.706774</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.149539  </td><td style=\"text-align: right;\"> 0.850461</td><td style=\"text-align: right;\"> 0.85357 </td><td>2023-04-05_11-32-30</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2295.08</td><td style=\"text-align: right;\">           210.576</td><td style=\"text-align: right;\">       2295.08</td><td style=\"text-align: right;\"> 1680661950</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>e0a0fca2  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_e5aa0d51</td><td style=\"text-align: right;\">  0.605864</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00566159</td><td style=\"text-align: right;\"> 0.798701</td><td style=\"text-align: right;\"> 0.785549</td><td>2023-04-05_09-03-12</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2295.23</td><td style=\"text-align: right;\">           212.349</td><td style=\"text-align: right;\">       2295.23</td><td style=\"text-align: right;\"> 1680652992</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>e5aa0d51  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_eab24463</td><td style=\"text-align: right;\">  0.72932 </td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.120983  </td><td style=\"text-align: right;\"> 0.862081</td><td style=\"text-align: right;\"> 0.879017</td><td>2023-04-03_19-03-21</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         8</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2587.93</td><td style=\"text-align: right;\">           210.434</td><td style=\"text-align: right;\">       2587.93</td><td style=\"text-align: right;\"> 1680516201</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>eab24463  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_ebfa5889</td><td style=\"text-align: right;\">  0.587254</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.00556477</td><td style=\"text-align: right;\"> 0.789188</td><td style=\"text-align: right;\"> 0.771967</td><td>2023-04-03_20-13-44</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             1403.01</td><td style=\"text-align: right;\">           210.153</td><td style=\"text-align: right;\">       1403.01</td><td style=\"text-align: right;\"> 1680520424</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>ebfa5889  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_ec5e8053</td><td style=\"text-align: right;\">  0.784119</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.109802  </td><td style=\"text-align: right;\"> 0.890198</td><td style=\"text-align: right;\"> 0.883184</td><td>2023-04-03_18-20-12</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             5018.32</td><td style=\"text-align: right;\">           210.693</td><td style=\"text-align: right;\">       5018.32</td><td style=\"text-align: right;\"> 1680513612</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>ec5e8053  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_ec7fc1ba</td><td style=\"text-align: right;\">  0.623376</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.191386  </td><td style=\"text-align: right;\"> 0.808614</td><td style=\"text-align: right;\"> 0.731546</td><td>2023-04-05_16-05-17</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2296.29</td><td style=\"text-align: right;\">           211.403</td><td style=\"text-align: right;\">       2296.29</td><td style=\"text-align: right;\"> 1680678317</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>ec7fc1ba  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_ed035e62</td><td style=\"text-align: right;\">  0.783123</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.110264  </td><td style=\"text-align: right;\"> 0.889736</td><td style=\"text-align: right;\"> 0.8841  </td><td>2023-04-04_18-24-28</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             5023.3 </td><td style=\"text-align: right;\">           211.074</td><td style=\"text-align: right;\">       5023.3 </td><td style=\"text-align: right;\"> 1680600268</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>ed035e62  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_f01daff2</td><td style=\"text-align: right;\">  0.707895</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0225777 </td><td style=\"text-align: right;\"> 0.851156</td><td style=\"text-align: right;\"> 0.855656</td><td>2023-04-05_11-54-38</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             1327.18</td><td style=\"text-align: right;\">           210.505</td><td style=\"text-align: right;\">       1327.18</td><td style=\"text-align: right;\"> 1680663278</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>f01daff2  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_f0d91e6e</td><td style=\"text-align: right;\">  0.692932</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0229761 </td><td style=\"text-align: right;\"> 0.843409</td><td style=\"text-align: right;\"> 0.855511</td><td>2023-04-05_03-45-04</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2293   </td><td style=\"text-align: right;\">           211.679</td><td style=\"text-align: right;\">       2293   </td><td style=\"text-align: right;\"> 1680633904</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>f0d91e6e  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_f1bc4de4</td><td style=\"text-align: right;\">  0.764582</td><td style=\"text-align: right;\">      8</td><td style=\"text-align: right;\">0.119724  </td><td style=\"text-align: right;\"> 0.880276</td><td style=\"text-align: right;\"> 0.874394</td><td>2023-04-05_15-27-01</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         8</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             3568.6 </td><td style=\"text-align: right;\">           211.396</td><td style=\"text-align: right;\">       3568.6 </td><td style=\"text-align: right;\"> 1680676021</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   8</td><td>f1bc4de4  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_f675a69b</td><td style=\"text-align: right;\">  0.741387</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.0698131 </td><td style=\"text-align: right;\"> 0.868366</td><td style=\"text-align: right;\"> 0.860043</td><td>2023-04-05_03-06-51</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2715.67</td><td style=\"text-align: right;\">           211.899</td><td style=\"text-align: right;\">       2715.67</td><td style=\"text-align: right;\"> 1680631611</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>f675a69b  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_f83d5cc3</td><td style=\"text-align: right;\">  0.652546</td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">0.0270634 </td><td style=\"text-align: right;\"> 0.822466</td><td style=\"text-align: right;\"> 0.842075</td><td>2023-04-04_07-52-45</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         2</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2283.32</td><td style=\"text-align: right;\">           210.368</td><td style=\"text-align: right;\">       2283.32</td><td style=\"text-align: right;\"> 1680562365</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   2</td><td>f83d5cc3  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_fa15857c</td><td style=\"text-align: right;\">  0.728518</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.128451  </td><td style=\"text-align: right;\"> 0.861696</td><td style=\"text-align: right;\"> 0.871549</td><td>2023-04-05_09-06-55</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2716.22</td><td style=\"text-align: right;\">           210.838</td><td style=\"text-align: right;\">       2716.22</td><td style=\"text-align: right;\"> 1680653215</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>fa15857c  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_fb0b6f4b</td><td style=\"text-align: right;\">  0.715301</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.144598  </td><td style=\"text-align: right;\"> 0.855402</td><td style=\"text-align: right;\"> 0.815404</td><td>2023-04-04_00-47-04</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             2739.96</td><td style=\"text-align: right;\">           210.439</td><td style=\"text-align: right;\">       2739.96</td><td style=\"text-align: right;\"> 1680536824</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>fb0b6f4b  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "<tr><td>train_fb6ff59d</td><td style=\"text-align: right;\">  0.748478</td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">0.128059  </td><td style=\"text-align: right;\"> 0.871941</td><td style=\"text-align: right;\"> 0.87602 </td><td>2023-04-05_08-24-56</td><td>True  </td><td>                </td><td>3885aed29cca4820a8c903630a03d206</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                         4</td><td>141.223.108.122</td><td style=\"text-align: right;\">37527</td><td>True               </td><td style=\"text-align: right;\">             2718.64</td><td style=\"text-align: right;\">           211.154</td><td style=\"text-align: right;\">       2718.64</td><td style=\"text-align: right;\"> 1680650696</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   4</td><td>fb6ff59d  </td><td style=\"text-align: right;\">    0.0299387</td></tr>\n",
       "<tr><td>train_fe6a3ba0</td><td style=\"text-align: right;\">  0.782818</td><td style=\"text-align: right;\">     15</td><td style=\"text-align: right;\">0.110406  </td><td style=\"text-align: right;\"> 0.889594</td><td style=\"text-align: right;\"> 0.880014</td><td>2023-04-04_10-38-28</td><td>True  </td><td>                </td><td>c4d33fd4afb84fc88a71d5bf14162e32</td><td>pirl-PowerEdge-T640</td><td style=\"text-align: right;\">                        15</td><td>141.223.108.122</td><td style=\"text-align: right;\">37443</td><td>True               </td><td style=\"text-align: right;\">             5011.13</td><td style=\"text-align: right;\">           211.112</td><td style=\"text-align: right;\">       5011.13</td><td style=\"text-align: right;\"> 1680572308</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>fe6a3ba0  </td><td style=\"text-align: right;\">    0.0315464</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 13:08:55,271\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n",
      "2023-04-03 13:10:52,517\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-03 13:31:07,508\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n",
      "2023-04-03 14:20:09,912\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': TverskyLoss}\n",
      "2023-04-03 14:54:49,961\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-03 15:32:47,959\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-03 15:44:10,563\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': TverskyLoss}\n",
      "2023-04-03 16:13:21,954\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-03 16:56:34,509\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-03 17:21:51,207\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': TverskyLoss}\n",
      "2023-04-03 17:45:11,682\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-03 18:07:15,670\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n",
      "2023-04-03 18:20:13,023\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-03 19:03:21,087\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': TverskyLoss}\n",
      "2023-04-03 19:16:32,661\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': TverskyLoss}\n",
      "2023-04-03 19:26:51,173\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': TverskyLoss}\n",
      "2023-04-03 19:50:21,868\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': TverskyLoss}\n",
      "2023-04-03 20:13:45,051\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-03 20:41:07,983\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': TverskyLoss}\n",
      "2023-04-03 21:12:49,714\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 22:04:46,237\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n",
      "2023-04-03 22:36:59,794\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n",
      "2023-04-03 23:28:25,509\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 00:01:24,087\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 00:47:04,192\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 00:51:57,079\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 02:10:44,349\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 02:15:29,402\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 03:34:50,905\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 03:39:58,572\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 04:34:01,330\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 05:04:13,972\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 05:58:08,636\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 06:03:15,618\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 06:32:13,636\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 06:36:28,390\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 07:10:20,705\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 07:14:42,411\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 07:52:45,919\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-04 08:14:48,029\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 08:33:57,131\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 08:52:54,847\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-04 09:14:57,612\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': TverskyLoss}\n",
      "2023-04-04 09:43:05,969\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 10:20:52,114\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': TverskyLoss}\n",
      "2023-04-04 10:38:28,951\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 10:44:07,202\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-04 11:00:33,149\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 11:21:56,415\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': TverskyLoss}\n",
      "2023-04-04 11:38:32,739\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-04 11:52:11,502\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 12:14:06,327\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 12:16:58,879\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': TverskyLoss}\n",
      "2023-04-04 12:47:55,212\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 12:51:53,528\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-04 13:33:14,111\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': TverskyLoss}\n",
      "2023-04-04 14:15:53,618\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 14:53:41,180\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-04 14:57:03,735\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 15:23:57,986\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 15:26:12,561\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': TverskyLoss}\n",
      "2023-04-04 16:10:58,949\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 16:22:56,429\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 16:49:27,331\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-04 17:00:44,863\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 17:27:25,869\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 18:24:28,396\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 18:26:32,536\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 19:48:25,660\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 19:50:13,878\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 20:35:23,440\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 21:12:25,546\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 21:59:07,277\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 22:12:06,709\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 22:37:15,796\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-04 23:19:46,399\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 23:49:58,263\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': TverskyLoss}\n",
      "2023-04-05 00:01:12,599\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 00:39:20,461\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-05 01:13:56,669\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 01:17:31,027\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 02:16:48,486\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': TverskyLoss}\n",
      "2023-04-05 02:21:35,319\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 03:06:51,207\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 03:25:51,524\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 03:45:04,421\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-05 04:04:03,807\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 04:23:20,081\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 04:33:18,795\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': TverskyLoss}\n",
      "2023-04-05 05:07:51,820\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 05:11:30,422\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-05 05:56:42,639\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 06:31:53,587\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 06:34:51,675\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': TverskyLoss}\n",
      "2023-04-05 06:58:12,667\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 07:36:24,037\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-05 07:39:38,073\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 08:21:39,231\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 08:24:56,937\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 09:03:12,396\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 09:06:55,664\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': TverskyLoss}\n",
      "2023-04-05 09:45:09,090\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-05 09:46:12,831\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 10:31:32,296\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 10:54:15,776\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 11:16:54,445\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': TverskyLoss}\n",
      "2023-04-05 11:32:31,089\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 11:54:38,503\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-05 11:55:11,104\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 12:40:33,567\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 12:53:59,313\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 13:03:57,323\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': TverskyLoss}\n",
      "2023-04-05 13:42:12,638\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 14:17:54,559\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 14:27:32,693\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': FocalTverskyLoss}\n",
      "2023-04-05 14:40:03,015\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.PixelLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': FrangiFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': FocalLoss, 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 15:25:20,082\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.BorderedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SumFilters', 'FrangiFilter', 'SatoFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': TverskyLoss}\n",
      "2023-04-05 15:27:01,539\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 15:48:46,950\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.CombinedLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': SatoFilter, 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n",
      "2023-04-05 16:05:18,077\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'Combine Loss': <class 'loss.loss.VolumeLoss'>, 'Input Filter': <function normalize at 0x7f4be3078af0>, 'Input Layer': ('SequenceFilters', 'SumFilters(FrangiFilter+SatoFilter)', 'BinaryFilter', 'SkeletonFilter'), 'Network': <class 'models.unet.UNet'>, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'Pixel Loss': CrossEntropyLoss(), 'Volume Loss': JaccardLoss}\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08aabe4",
   "metadata": {},
   "source": [
    "# Evaluating the best Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa2da49",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-03T02:44:02.002Z"
    }
   },
   "outputs": [],
   "source": [
    "best_result = results.get_best_result(metric=\"CrackIoU\", mode=\"max\", scope=\"all\")  # Get best result object\n",
    "print(\"Best trial config: {}\".format(best_result.config))\n",
    "print(\"Best trial final validation loss: {}\".format(best_result.metrics[\"Loss\"]))\n",
    "print(\"Best trial final validation CrackIoU: {}\".format(best_result.metrics[\"CrackIoU\"]))\n",
    "\n",
    "for result in results:\n",
    "    evaluate(test_data=test_data, result=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8633d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599c84a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "POC-env",
   "language": "python",
   "name": "poc-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.844,
   "position": {
    "height": "40px",
    "left": "1576px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
