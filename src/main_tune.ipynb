{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25a1ac32",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setting-up-imports\" data-toc-modified-id=\"Setting-up-imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setting up imports</a></span></li><li><span><a href=\"#Setting-up-Constant-Hyperparameters\" data-toc-modified-id=\"Setting-up-Constant-Hyperparameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setting up Constant Hyperparameters</a></span></li><li><span><a href=\"#Setting-up-Parameters-and-Functions-for-Training\" data-toc-modified-id=\"Setting-up-Parameters-and-Functions-for-Training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Setting up Parameters and Functions for Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameters-Search-Space\" data-toc-modified-id=\"Hyperparameters-Search-Space-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Hyperparameters Search Space</a></span></li><li><span><a href=\"#Creating-the-training-function\" data-toc-modified-id=\"Creating-the-training-function-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Creating the training function</a></span></li><li><span><a href=\"#Creating-the-evaluation-function\" data-toc-modified-id=\"Creating-the-evaluation-function-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Creating the evaluation function</a></span></li></ul></li><li><span><a href=\"#Running-the-training\" data-toc-modified-id=\"Running-the-training-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Running the training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-data-for-training\" data-toc-modified-id=\"Loading-data-for-training-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Loading data for training</a></span></li><li><span><a href=\"#Configuring-the-Tuner-with-a-Scheduler-and-a-Search-Algorithm\" data-toc-modified-id=\"Configuring-the-Tuner-with-a-Scheduler-and-a-Search-Algorithm-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Configuring the Tuner with a Scheduler and a Search Algorithm</a></span></li><li><span><a href=\"#Running-the-Tuner\" data-toc-modified-id=\"Running-the-Tuner-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Running the Tuner</a></span></li></ul></li><li><span><a href=\"#Evaluating-the-best-Results\" data-toc-modified-id=\"Evaluating-the-best-Results-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluating the best Results</a></span></li></ul></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "035da606",
   "metadata": {},
   "source": [
    "# Setting up imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16e0d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T06:42:03.399585Z",
     "start_time": "2023-05-15T06:42:00.552671Z"
    },
    "cell_style": "center",
    "init_cell": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss, Sequential\n",
    "from torch.nn.functional import normalize\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import CenterCrop, Resize, RandomCrop, GaussianBlur\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import session, RunConfig, CheckpointConfig\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "\n",
    "\n",
    "from dataset import POCDataReader, POCvsCS9DataReader, data_augment_, POCDataset\n",
    "from metrics import Metrics, EvaluationMetrics\n",
    "from models import UNet, DeepCrack, SubUNet, DenSubUNet\n",
    "from loss import *\n",
    "from pipelines import InputPipeline\n",
    "from pipelines.filters import CrackBinaryFilter, BGBinaryFilter, SequenceFilters, SumFilters\n",
    "# from pipelines.filters.small_kernel import FrangiFilter, LaplacianFilter, SatoFilter, SobelFilter\n",
    "from pipelines.filters.medium_kernel import FrangiFilter, LaplacianFilter, SatoFilter, SobelFilter\n",
    "# from pipelines.filters.large_kernel import FrangiFilter, LaplacianFilter, SatoFilter, SobelFilter\n",
    "from train import training_loop, validation_loop\n",
    "from train_tqdm import evaluation_loop\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d9951dd",
   "metadata": {},
   "source": [
    "# Setting up Constant Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb97ae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T06:42:03.405701Z",
     "start_time": "2023-05-15T06:42:03.402426Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "NUM_SAMPLES = 1\n",
    "\n",
    "NUM_AUGMENT = 1\n",
    "\n",
    "LOAD_DATA_ON_GPU = True\n",
    "GPUS_PER_TRIAL = 1\n",
    "CPUS_PER_TRIAL = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d48f809",
   "metadata": {},
   "source": [
    "##### Selecting Cuda device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6be70b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T06:42:03.590395Z",
     "start_time": "2023-05-15T06:42:03.407534Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9687c3e3",
   "metadata": {},
   "source": [
    "# Setting up Parameters and Functions for Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "215c221d",
   "metadata": {},
   "source": [
    "## Hyperparameters Search Space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d931716",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Preload Losses Functions\n",
    "Get a list of all loss function per type (Pixel/Volume) for grid or random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28766a0c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "pixel_loss_list = [\n",
    "    CrossEntropyLoss(weight=torch.tensor([.65, .35])),\n",
    "    FocalLoss(weight=torch.tensor([.65, .35]), gamma=1.4),\n",
    "]\n",
    "\n",
    "volume_loss_list = [\n",
    "    JaccardLoss(),\n",
    "    TverskyLoss(alpha=0.3, beta=0.7),\n",
    "    FocalTverskyLoss(alpha=0.3, beta=0.7, gamma=1.4),\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9939e19b",
   "metadata": {},
   "source": [
    "##### Preload Pipeline\n",
    "Get a list of all possible filter to apply in the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e663e37",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "filter_list = [normalize]\n",
    "\n",
    "layer_list = [\n",
    "    None,\n",
    "    SobelFilter(),\n",
    "    LaplacianFilter(threshold=0.75),\n",
    "    FrangiFilter(),\n",
    "    SatoFilter(),\n",
    "    SumFilters(FrangiFilter(), SatoFilter()),\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83d95e76",
   "metadata": {},
   "source": [
    "##### Search Space\n",
    "A dict containing all hyperparameters that we want to analyse and try (also put constant ones in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e136d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T06:42:03.598097Z",
     "start_time": "2023-05-15T06:42:03.593653Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"Network\": tune.grid_search([UNet, DeepCrack, SubUNet]),\n",
    "    \"Optimizer\": Adam,\n",
    "\n",
    "    \"Learning Rate\": 1e-4, #tune.loguniform(1e-6, 1e-3),\n",
    "    \"Batch Size\": 16,\n",
    "\n",
    "    \"Loss Combiner\": tune.grid_search([MeanLoss, PixelLoss, VolumeLoss]),\n",
    "    \"Loss Combiner_ratio\": tune.grid_search([0, .25, .5, .75, 1]),\n",
    "    \"Loss Volume\": tune.grid_search(volume_loss_list),\n",
    "    \"Loss Pixel\": tune.grid_search(pixel_loss_list),\n",
    "\n",
    "    \"Pipe Filter\": normalize,\n",
    "    \"Pipe Layer\": tune.grid_search(layer_list),\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecc521e6",
   "metadata": {},
   "source": [
    "## Creating the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626acb32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T06:42:03.614954Z",
     "start_time": "2023-05-15T06:42:03.600176Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def train(config, train_data, val_data):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    inpip = InputPipeline(\n",
    "        filter=config[\"Pipe Filter\"],\n",
    "        additional_channel=config[\"Pipe Layer\"])\n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        inpip = inpip.to(device)\n",
    "\n",
    "    train_dataset = POCDataset(\n",
    "        data=train_data,\n",
    "        transform=inpip,\n",
    "        target_transform=None,\n",
    "        negative_mining=False,\n",
    "        load_on_gpu=LOAD_DATA_ON_GPU)\n",
    "    train_dataset.precompute_transform()\n",
    "\n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        training_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=int(config[\"Batch Size\"]),\n",
    "            sampler=train_dataset.sampler,\n",
    "            shuffle= True if train_dataset.sampler is None else None,\n",
    "        )\n",
    "    else:\n",
    "        training_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=int(config[\"Batch Size\"]),\n",
    "            sampler=train_dataset.sampler,\n",
    "            shuffle= True if train_dataset.sampler is None else None,\n",
    "            num_workers=CPUS_PER_TRIAL//2,\n",
    "            pin_memory=True,\n",
    "            pin_memory_device=device)\n",
    "\n",
    "    val_dataset = POCDataset(\n",
    "        data=val_data, \n",
    "        transform=inpip,\n",
    "        target_transform=None,\n",
    "        negative_mining=False,\n",
    "        load_on_gpu=LOAD_DATA_ON_GPU)\n",
    "    val_dataset.precompute_transform()\n",
    "\n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=int(config[\"Batch Size\"]),\n",
    "            shuffle=True)\n",
    "    else:\n",
    "        validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=int(config[\"Batch Size\"]),\n",
    "            shuffle=True,\n",
    "            num_workers=CPUS_PER_TRIAL//2,\n",
    "            pin_memory=True,\n",
    "            pin_memory_device=device)\n",
    "\n",
    "    model = config[\"Network\"](n_channels=inpip.nb_channel, n_classes=2)\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = MultiscaleLoss(config[\"Loss Combiner\"](\n",
    "        config[\"Loss Pixel\"],\n",
    "        config[\"Loss Volume\"],\n",
    "        ratio=config[\"Loss Combiner_ratio\"])).to(device)\n",
    "\n",
    "    optimizer = config[\"Optimizer\"](model.parameters(), lr=config[\"Learning Rate\"], betas=(0.9, 0.99))\n",
    "    lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS//3)\n",
    "\n",
    "    loaded_checkpoint = session.get_checkpoint()\n",
    "    if loaded_checkpoint:\n",
    "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "            model_state, optimizer_state, scheduler_state = torch.load(os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\"))\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "        lr_scheduler.load_state_dict(scheduler_state)\n",
    "\n",
    "    train_metrics = Metrics(\n",
    "        buffer_size=len(training_dataloader),\n",
    "        mode=\"Training\",\n",
    "        hyperparam=config,\n",
    "        device=device)\n",
    "\n",
    "    val_metrics = Metrics(\n",
    "        buffer_size=len(validation_dataloader),\n",
    "        mode=\"Validation\",\n",
    "        hyperparam=config,\n",
    "        device=device)\n",
    "\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):  # loop over the dataset multiple times\n",
    "        training_loop(epoch, training_dataloader, model, loss_fn, optimizer, lr_scheduler, train_metrics, device)\n",
    "        validation_loop(epoch, validation_dataloader, model, loss_fn, val_metrics, device)\n",
    "\n",
    "        # Here we save a checkpoint. It is automatically registered with\n",
    "        # Ray Tune and can be accessed through `session.get_checkpoint()`\n",
    "        # API in future iterations.\n",
    "        os.makedirs(\"model\", exist_ok=True)\n",
    "        torch.save((model.state_dict(), optimizer.state_dict(), lr_scheduler.state_dict()), \"model/checkpoint.pt\")\n",
    "        checkpoint = Checkpoint.from_directory(\"model\")\n",
    "        session.report(metrics=val_metrics.get_metrics(epoch), checkpoint=checkpoint)\n",
    "\n",
    "    train_metrics.close_tensorboard()\n",
    "    val_metrics.close_tensorboard()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad8a58ec",
   "metadata": {},
   "source": [
    "## Creating the evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d17c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T06:42:03.622812Z",
     "start_time": "2023-05-15T06:42:03.616733Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def evaluate(test_data, result):\n",
    "\n",
    "    if not result.best_checkpoints:\n",
    "        return None\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    inpip = InputPipeline(\n",
    "        filter=result.config[\"Pipe Filter\"],\n",
    "        additional_channel=result.config[\"Pipe Layer\"])\n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        inpip = inpip.to(device)\n",
    "\n",
    "    test_dataset = POCDataset(\n",
    "        test_data,\n",
    "        transform=inpip,\n",
    "        target_transform=None,\n",
    "        negative_mining=False,\n",
    "        load_on_gpu=LOAD_DATA_ON_GPU)\n",
    "    \n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        evaluation_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "    else:\n",
    "        evaluation_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=2*CPUS_PER_TRIAL, pin_memory=True, pin_memory_device=device)\n",
    "\n",
    "    best_trained_model = result.config[\"Network\"](n_channels=inpip.nb_channel, n_classes=2).to(device)\n",
    "\n",
    "    checkpoint_path = os.path.join(result.best_checkpoints[0][0].to_directory(), \"checkpoint.pt\")\n",
    "    model_state, _, _ = torch.load(checkpoint_path)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    test_metrics = EvaluationMetrics(\n",
    "        buffer_size=len(evaluation_dataloader),\n",
    "        hyperparam=result.config,\n",
    "        epochs=result.best_checkpoints[0][1][\"Epoch\"], # True epoch of the best run\n",
    "        device=device)\n",
    "\n",
    "    evaluation_loop(dataloader=evaluation_dataloader, model=best_trained_model, metric=test_metrics, device=device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "918e1c03",
   "metadata": {},
   "source": [
    "# Running the training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f955b19",
   "metadata": {},
   "source": [
    "## Loading data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742acf4f",
   "metadata": {},
   "source": [
    "##### Loading POC2 dataset for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a7a2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T06:42:59.555200Z",
     "start_time": "2023-05-15T06:42:03.624557Z"
    }
   },
   "outputs": [],
   "source": [
    "data_reader = POCDataReader(root_dir=\"../data/POC\", load_on_gpu=False, verbose=True)\n",
    "train_data, val_data, test_data = data_reader.split()\n",
    "\n",
    "train_data = data_augment_(train_data, n=NUM_AUGMENT, load_on_gpu=False, verbose=True, seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0e0fe",
   "metadata": {},
   "source": [
    "##### OR Loading POC2 or CS9 dataset for training comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_reader = POCvsCS9DataReader(root_dir=\"../data/POCvsCS9\", dataset=\"cs9\", load_on_gpu=False, verbose=True)\n",
    "# train_data, val_data, test_data = data_reader.split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "025b0faa",
   "metadata": {},
   "source": [
    "## Configuring the Tuner with a Scheduler and a Search Algorithm (Using the ray tune library)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca91a5b",
   "metadata": {},
   "source": [
    "##### Create a new Tune experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69701ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T06:43:07.962177Z",
     "start_time": "2023-05-15T06:42:59.558654Z"
    }
   },
   "outputs": [],
   "source": [
    "# scheduler = ASHAScheduler(max_t=EPOCHS, grace_period=2, reduction_factor=2)\n",
    "# search_algo = HyperOptSearch()\n",
    "# search_algo = OptunaSearch()\n",
    "\n",
    "tune_config = tune.TuneConfig(\n",
    "    metric=\"CrackIoU\",\n",
    "    mode=\"max\",\n",
    "    num_samples=NUM_SAMPLES,\n",
    "#     scheduler=scheduler,\n",
    "#     search_alg=search_algo,\n",
    "    max_concurrent_trials=4,\n",
    ")\n",
    "\n",
    "tune_trainable = tune.with_resources(\n",
    "    trainable=tune.with_parameters(train, train_data=train_data, val_data=val_data),\n",
    "    resources={\"cpu\": CPUS_PER_TRIAL, \"gpu\": GPUS_PER_TRIAL}\n",
    ")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable=tune_trainable,\n",
    "    tune_config=tune_config,\n",
    "    param_space=search_space,\n",
    "    run_config=RunConfig(\n",
    "        local_dir=\"~/POC-Project/ray_results/\",\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"CrackIoU\",\n",
    "            checkpoint_score_order=\"max\",\n",
    "            checkpoint_at_end=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9867b09f",
   "metadata": {},
   "source": [
    "##### Or load a previous one from disk (with its results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efab3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner = tune.Tuner.restore(\n",
    "#     path=\"/home/pirl/POC-Project/ray_results/train_2023-06-30_18-36-49_small_kernel/\",\n",
    "#     trainable=None, #tune_trainable,\n",
    "#     resume_unfinished=False,\n",
    "#     resume_errored=False,\n",
    "#     restart_errored=False,\n",
    "# )\n",
    "\n",
    "# results = tuner.get_results()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b0913de",
   "metadata": {},
   "source": [
    "## Running the Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb3720",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T04:16:39.713267Z",
     "start_time": "2023-05-15T06:43:07.964985Z"
    }
   },
   "outputs": [],
   "source": [
    "results = tuner.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f08aabe4",
   "metadata": {},
   "source": [
    "# Evaluating the best Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa2da49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T04:17:37.361539Z",
     "start_time": "2023-05-17T04:16:39.717794Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_result = results.get_best_result(metric=\"CrackIoU\", mode=\"max\", scope=\"all\")  # Get best result object\n",
    "print(\"Best trial config: {}\".format(best_result.config))\n",
    "print(\"Best trial final validation loss: {}\".format(best_result.metrics[\"Loss\"]))\n",
    "print(\"Best trial final validation CrackIoU: {}\".format(best_result.metrics[\"CrackIoU\"]))\n",
    "\n",
    "for result in results:\n",
    "    evaluate(test_data=test_data, result=result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a0e00c6",
   "metadata": {},
   "source": [
    "## Gathering activation maps & predictions from the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b07187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_utils import show_img\n",
    "    \n",
    "def print_activation_map(result, test_data):\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    inpip = InputPipeline(\n",
    "        filter=result.config[\"Pipe Filter\"],\n",
    "        additional_channel=result.config[\"Pipe Layer\"])\n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        inpip = inpip.to(device)\n",
    "\n",
    "    test_dataset = POCDataset(\n",
    "        test_data,\n",
    "        transform=inpip,\n",
    "        target_transform=None,\n",
    "        negative_mining=False,\n",
    "        load_on_gpu=LOAD_DATA_ON_GPU)\n",
    "\n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        evaluation_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    else:\n",
    "        evaluation_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2*CPUS_PER_TRIAL, pin_memory=True, pin_memory_device=device)\n",
    "\n",
    "    images = next(iter(evaluation_dataloader))[0]\n",
    "\n",
    "    best_trained_model = result.config[\"Network\"](n_channels=inpip.nb_channel, n_classes=2).to(device)\n",
    "\n",
    "    checkpoint_path = os.path.join(result.best_checkpoints[0][0].to_directory(), \"checkpoint.pt\")\n",
    "    model_state, _, _ = torch.load(checkpoint_path)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    \n",
    "    best_trained_model.eval()\n",
    "    first_block = best_trained_model.encoder.block1\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        activation_map = first_block(images)[0].sum(dim=1, keepdim=True)\n",
    "\n",
    "    show_img(images)\n",
    "    show_img(activation_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da340551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_utils import show_img\n",
    "    \n",
    "def print_prediction_proba(result, test_data):\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    inpip = InputPipeline(\n",
    "        filter=result.config[\"Pipe Filter\"],\n",
    "        additional_channel=result.config[\"Pipe Layer\"])\n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        inpip = inpip.to(device)\n",
    "\n",
    "    test_dataset = POCDataset(\n",
    "        test_data,\n",
    "        transform=inpip,\n",
    "        target_transform=None,\n",
    "        negative_mining=False,\n",
    "        load_on_gpu=LOAD_DATA_ON_GPU)\n",
    "\n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        evaluation_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "    else:\n",
    "        evaluation_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=2*CPUS_PER_TRIAL, pin_memory=True, pin_memory_device=device)\n",
    "\n",
    "    images = next(iter(evaluation_dataloader))[0]\n",
    "\n",
    "    best_trained_model = result.config[\"Network\"](n_channels=inpip.nb_channel, n_classes=2).to(device)\n",
    "\n",
    "    checkpoint_path = os.path.join(result.best_checkpoints[0][0].to_directory(), \"checkpoint.pt\")\n",
    "    model_state, _, _ = torch.load(checkpoint_path)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    \n",
    "    best_trained_model.eval()\n",
    "    with torch.inference_mode():\n",
    "        prediction_proba = best_trained_model(images)\n",
    "\n",
    "        heatmap = images.clone().detach()\n",
    "        heatmap /= heatmap.max()\n",
    "        heatmap[:,1] += .1 * prediction_proba[:,1]\n",
    "\n",
    "    show_img(heatmap)\n",
    "    show_img(prediction_proba[:,1:])\n",
    "    show_img(prediction_proba.argmax(dim=1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb729ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = 'cs9'\n",
    "\n",
    "def get_visualisation_files(result, test_data):\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    inpip = InputPipeline(\n",
    "        filter=result.config[\"Pipe Filter\"],\n",
    "        additional_channel=result.config[\"Pipe Layer\"])\n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        inpip = inpip.to(device)\n",
    "\n",
    "    test_dataset = POCDataset(\n",
    "        test_data,\n",
    "        transform=inpip,\n",
    "        target_transform=None,\n",
    "        negative_mining=False,\n",
    "        load_on_gpu=LOAD_DATA_ON_GPU)\n",
    "\n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        evaluation_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    else:\n",
    "        evaluation_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2*CPUS_PER_TRIAL, pin_memory=True, pin_memory_device=device)\n",
    "\n",
    "    best_trained_model = result.config[\"Network\"](n_channels=inpip.nb_channel, n_classes=2).to(device)\n",
    "\n",
    "    checkpoint_path = os.path.join(result.best_checkpoints[0][0].to_directory(), \"checkpoint.pt\")\n",
    "    model_state, _, _ = torch.load(checkpoint_path)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    \n",
    "    best_trained_model.eval()\n",
    "    first_block = best_trained_model.encoder.block1\n",
    "\n",
    "    # data_iterator = iter(evaluation_dataloader)\n",
    "    # list_first_data = [next(data_iterator) for _ in range(10)]\n",
    "    for item in evaluation_dataloader:\n",
    "        image = item[0]; label = item[1]; fname = item[2][0]\n",
    "        fpath = f\"../imgs_net/{fname}\"\n",
    "\n",
    "        if not os.path.exists(fpath):\n",
    "            os.makedirs(fpath+\"/activation_maps\")\n",
    "            os.makedirs(fpath+\"/heatmap\")\n",
    "            os.makedirs(fpath+\"/mask_bin\")\n",
    "\n",
    "        if not os.path.exists(fpath + \"/img.png\"):\n",
    "            img = image[0,:3].clone()\n",
    "            img -= img.min(); img /= img.max()\n",
    "            # print(img.size(), img.unique())\n",
    "            save_image(img, f\"{fpath}/img.png\")\n",
    "        if not os.path.exists(fpath + \"/label.png\"):\n",
    "            label = label[0,1:].expand(3,-1,-1)\n",
    "            # print(label.size(), label.unique())\n",
    "            save_image(label, f\"{fpath}/label.png\")\n",
    "\n",
    "        best_trained_model.eval()\n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            # if str(result.config['Pipe Layer']) != \"None\":\n",
    "            #     save_image(image[0,3:].expand(3, -1, -1), f\"{fpath}/filter/{dataset}.png\")\n",
    "\n",
    "            activation_map = first_block(image)[0].sum(dim=1, keepdim=True)[0].expand(3, -1, -1).clone()\n",
    "            activation_map -= activation_map.min()\n",
    "            activation_map /= activation_map.max()\n",
    "            # print(activation_map.size(), activation_map.unique())\n",
    "            save_image(activation_map, f\"{fpath}/activation_maps/{dataset}.png\")\n",
    "\n",
    "            prediction_proba = best_trained_model(image)[0]\n",
    "            \n",
    "            heatmap = image.clone().detach()[0,:3]\n",
    "            heatmap /= heatmap.max()\n",
    "            heatmap[1] += .5 * prediction_proba[1]\n",
    "            # print(heatmap.size(), heatmap.unique())\n",
    "            save_image(heatmap, f\"{fpath}/heatmap/{dataset}.png\")\n",
    "            \n",
    "            mask_bin = prediction_proba.argmax(dim=0, keepdim=True).expand(3, -1, -1).float()\n",
    "            # print(mask_bin.size(), mask_bin.unique())\n",
    "            save_image(mask_bin, f\"{fpath}/mask_bin/{dataset}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3562957",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    # print(f\"{result.config['Loss Combiner']} - {result.config['Pipe Layer']}\")\n",
    "    # print_activation_map(result, test_data)\n",
    "    # print_prediction_proba(result, test_data)\n",
    "    get_visualisation_files(result, test_data)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "POC-env",
   "language": "python",
   "name": "poc-env"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.844,
   "position": {
    "height": "144.844px",
    "left": "1576px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
