{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a1ac32",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setting-up-imports\" data-toc-modified-id=\"Setting-up-imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setting up imports</a></span></li><li><span><a href=\"#Setting-up-Constant-Hyperparameters\" data-toc-modified-id=\"Setting-up-Constant-Hyperparameters-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setting up Constant Hyperparameters</a></span></li><li><span><a href=\"#Setting-up-Parameters-and-Functions-for-Training\" data-toc-modified-id=\"Setting-up-Parameters-and-Functions-for-Training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Setting up Parameters and Functions for Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameters-Search-Space\" data-toc-modified-id=\"Hyperparameters-Search-Space-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Hyperparameters Search Space</a></span></li><li><span><a href=\"#Creating-the-training-function\" data-toc-modified-id=\"Creating-the-training-function-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Creating the training function</a></span></li><li><span><a href=\"#Creating-the-evaluation-function\" data-toc-modified-id=\"Creating-the-evaluation-function-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Creating the evaluation function</a></span></li></ul></li><li><span><a href=\"#Running-the-training\" data-toc-modified-id=\"Running-the-training-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Running the training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-data-for-training\" data-toc-modified-id=\"Loading-data-for-training-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Loading data for training</a></span></li><li><span><a href=\"#Configuring-the-Tuner-with-a-Scheduler-and-a-Search-Algorithm\" data-toc-modified-id=\"Configuring-the-Tuner-with-a-Scheduler-and-a-Search-Algorithm-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Configuring the Tuner with a Scheduler and a Search Algorithm</a></span></li><li><span><a href=\"#Running-the-Tuner\" data-toc-modified-id=\"Running-the-Tuner-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Running the Tuner</a></span></li></ul></li><li><span><a href=\"#Evaluating-the-best-Results\" data-toc-modified-id=\"Evaluating-the-best-Results-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluating the best Results</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035da606",
   "metadata": {},
   "source": [
    "# Setting up imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af16e0d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T03:57:05.595174Z",
     "start_time": "2023-03-17T03:57:03.191392Z"
    },
    "cell_style": "center",
    "init_cell": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.functional import normalize\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import GaussianBlur\n",
    "from torchvision.transforms.functional import invert\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "\n",
    "from Dataset import POCDataReader, data_augment_, POCDataset\n",
    "from metrics import Metrics, EvaluationMetrics\n",
    "from models import UNet\n",
    "from loss import *\n",
    "from pipelines import *\n",
    "from train import training_loop, validation_loop\n",
    "from train_tqdm import evaluation_loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9951dd",
   "metadata": {},
   "source": [
    "# Setting up Constant Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb97ae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T03:57:05.600487Z",
     "start_time": "2023-03-17T03:57:05.597798Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 12\n",
    "NUM_SAMPLES = 30\n",
    "NUM_MODEL_TEST = 5\n",
    "\n",
    "NUM_AUGMENT = 1\n",
    "\n",
    "LOAD_DATA_ON_GPU = True\n",
    "GPUS_PER_TRIAL = 1\n",
    "CPUS_PER_TRIAL = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48f809",
   "metadata": {},
   "source": [
    "##### Selecting Cuda device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd6be70b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T03:57:05.703921Z",
     "start_time": "2023-03-17T03:57:05.602011Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9687c3e3",
   "metadata": {},
   "source": [
    "# Setting up Parameters and Functions for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580710fe",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Setting up the loss function sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b7f919",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T02:58:33.864244Z",
     "start_time": "2023-03-17T02:58:33.860388Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "hidden": true,
    "init_cell": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn_sampler():\n",
    "    pixel_losses_list = [\n",
    "        CrossEntropyLoss(weight=torch.tensor([.3, .7])), \n",
    "        FocalLoss(weight=torch.tensor([.3, .7]), gamma=2)\n",
    "    ]\n",
    "    volume_losses_list = [\n",
    "        JaccardLoss(),\n",
    "        TverskyLoss(alpha=0.3, beta=0.7),\n",
    "        FocalTverskyLoss(alpha=0.3, beta=0.7, gamma=2)\n",
    "    ]\n",
    "    loss_combinators_list = [ CombinedLoss, BorderedLoss ]\n",
    "\n",
    "    complete_list = pixel_losses_list + volume_losses_list\n",
    "\n",
    "    for combinator in loss_combinators_list:\n",
    "        complete_list += [combinator(loss1, loss2) for loss1 in pixel_losses_list for loss2 in volume_losses_list]\n",
    "\n",
    "    return complete_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749dfb8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T02:07:15.655414Z",
     "start_time": "2023-03-17T02:07:15.650854Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "##### Setting up the input Pipeline sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4efbeca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T02:58:33.868633Z",
     "start_time": "2023-03-17T02:58:33.865695Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "hidden": true,
    "init_cell": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def input_pip_sampler():\n",
    "    transformers_list = [\n",
    "        [invert, normalize],\n",
    "        [normalize],\n",
    "    ]\n",
    "    layer_transformers_list = [\n",
    "        None,\n",
    "        LaplacianFilter(),\n",
    "        SobelFilter(),\n",
    "    ]\n",
    "\n",
    "    return [InputPipeline(transformer, layer_transformer).to(device) for transformer in transformers_list for layer_transformer in layer_transformers_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c221d",
   "metadata": {},
   "source": [
    "## Hyperparameters Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1e136d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T03:57:06.052094Z",
     "start_time": "2023-03-17T03:57:05.708597Z"
    },
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OnlyPixelLoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m search_space \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1e-4\u001b[39m, \u001b[38;5;66;03m#tune.qloguniform(1e-5, 1e-2, 5e-6),\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;66;03m#tune.qrandint(2, 8, 2),\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNM\u001b[39m\u001b[38;5;124m\"\u001b[39m: tune\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m]),\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSL\u001b[39m\u001b[38;5;124m\"\u001b[39m: tune\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m]),\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_pixel\u001b[39m\u001b[38;5;124m\"\u001b[39m: tune\u001b[38;5;241m.\u001b[39mchoice([\n\u001b[1;32m      8\u001b[0m         CrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m.3\u001b[39m, \u001b[38;5;241m.7\u001b[39m])), \n\u001b[1;32m      9\u001b[0m         FocalLoss(weight\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m.3\u001b[39m, \u001b[38;5;241m.7\u001b[39m]), gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)]),\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_volume\u001b[39m\u001b[38;5;124m\"\u001b[39m: tune\u001b[38;5;241m.\u001b[39mchoice([\n\u001b[1;32m     11\u001b[0m         JaccardLoss(),\n\u001b[1;32m     12\u001b[0m         TverskyLoss(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m),\n\u001b[1;32m     13\u001b[0m         FocalTverskyLoss(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)]),\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_combinator\u001b[39m\u001b[38;5;124m\"\u001b[39m: tune\u001b[38;5;241m.\u001b[39mchoice([CombinedLoss, BorderedLoss, \u001b[43mOnlyPixelLoss\u001b[49m, OnlyVolumeLoss]),\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_filter\u001b[39m\u001b[38;5;124m\"\u001b[39m: tune\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;28;01mNone\u001b[39;00m, invert]),\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_layer\u001b[39m\u001b[38;5;124m\"\u001b[39m: tune\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;28;01mNone\u001b[39;00m, LaplacianFilter(), SobelFilter()]),\n\u001b[1;32m     18\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OnlyPixelLoss' is not defined"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    \"lr\": 1e-4, #tune.qloguniform(1e-5, 1e-2, 5e-6),\n",
    "    \"batch_size\": 4, #tune.qrandint(2, 8, 2),\n",
    "    \"NM\": tune.choice([True, False]),\n",
    "    \"SL\": tune.choice([True, False]),\n",
    "\n",
    "    \"loss_pixel\": tune.choice([\n",
    "        CrossEntropyLoss(weight=torch.tensor([.3, .7])), \n",
    "        FocalLoss(weight=torch.tensor([.3, .7]), gamma=2)]),\n",
    "    \"loss_volume\": tune.choice([\n",
    "        JaccardLoss(),\n",
    "        TverskyLoss(alpha=0.3, beta=0.7),\n",
    "        FocalTverskyLoss(alpha=0.3, beta=0.7, gamma=2)]),\n",
    "    \"loss_combinator\": tune.choice([CombinedLoss, BorderedLoss, OnlyPixelLoss, OnlyVolumeLoss]),\n",
    "\n",
    "    \"input_filter\": tune.choice([None, invert]),\n",
    "    \"input_layer\": tune.choice([None, LaplacianFilter(), SobelFilter()]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc521e6",
   "metadata": {},
   "source": [
    "## Creating the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626acb32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T03:57:06.055770Z",
     "start_time": "2023-03-17T03:57:06.055757Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def train(config, train_data, val_data):\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    inpip = InputPipeline(\n",
    "        transformer=[normalize, config[\"input_filter\"]] if config[\"input_filter\"] is not None else normalize, \n",
    "        layer_transformer=config[\"input_layer\"]).to(device)\n",
    "\n",
    "    train_dataset = POCDataset(\n",
    "        train_data,\n",
    "        transform=inpip,\n",
    "        target_transform= GaussianBlur(kernel_size=3, sigma=0.7) if config[\"SL\"] else None,\n",
    "        negative_mining=config[\"NM\"])\n",
    "\n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        training_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=int(config[\"batch_size\"]),\n",
    "            sampler=train_dataset.sampler)\n",
    "    else:\n",
    "        training_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=int(config[\"batch_size\"]),\n",
    "            sampler=train_dataset.sampler,\n",
    "            num_workers=CPUS_PER_TRIAL//2,\n",
    "            pin_memory=True,\n",
    "            pin_memory_device=device)\n",
    "\n",
    "    val_dataset = POCDataset(val_data, transform=inpip, target_transform=None, negative_mining=False)\n",
    "    \n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=int(config[\"batch_size\"]),\n",
    "            shuffle=True)\n",
    "    else:\n",
    "        validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=int(config[\"batch_size\"]),\n",
    "            shuffle=True,\n",
    "            num_workers=CPUS_PER_TRIAL//2,\n",
    "            pin_memory=True,\n",
    "            pin_memory_device=device)\n",
    "\n",
    "    model = UNet(n_channels=inpip.nb_channel, n_classes=2, bilinear=True, crop=False)\n",
    "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = config[\"loss_combinator\"](config[\"loss_pixel\"], config[\"loss_volume\"]).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=config[\"lr\"], betas=(0.9, 0.99))\n",
    "    lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS//2)\n",
    "\n",
    "    # To restore a checkpoint, use `session.get_checkpoint()`.\n",
    "    loaded_checkpoint = session.get_checkpoint()\n",
    "    if loaded_checkpoint:\n",
    "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "            model_state, optimizer_state, scheduler_state = torch.load(os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\"))\n",
    "        model.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "        lr_scheduler.load_state_dict(scheduler_state)\n",
    "\n",
    "    train_metrics = Metrics(\n",
    "        buffer_size=len(training_dataloader),\n",
    "        mode=\"Training\",\n",
    "        model_name=model.__class__.__name__,\n",
    "        loss_name=loss_fn.__class__.__name__,\n",
    "        opt_name=optimizer.__class__.__name__,\n",
    "        pip_name=str(config[\"input_pipeline\"]),\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        learning_rate=config[\"lr\"],\n",
    "        negative_mining=config[\"NM\"],\n",
    "        soft_labels=config[\"SL\"],\n",
    "        device=device)\n",
    "\n",
    "    val_metrics = Metrics(\n",
    "        buffer_size=len(validation_dataloader),\n",
    "        mode=\"Validation\",\n",
    "        model_name=model.__class__.__name__,\n",
    "        loss_name=loss_fn.__class__.__name__,\n",
    "        opt_name=optimizer.__class__.__name__,\n",
    "        pip_name=str(config[\"input_pipeline\"]),\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        learning_rate=config[\"lr\"],\n",
    "        negative_mining=config[\"NM\"],\n",
    "        soft_labels=config[\"SL\"],\n",
    "        device=device)\n",
    "\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):  # loop over the dataset multiple times\n",
    "        training_loop(epoch, training_dataloader, model, loss_fn, optimizer, lr_scheduler, train_metrics, device)\n",
    "        validation_loop(epoch, validation_dataloader, model, loss_fn, val_metrics, device)\n",
    "\n",
    "        # Here we save a checkpoint. It is automatically registered with\n",
    "        # Ray Tune and can be accessed through `session.get_checkpoint()`\n",
    "        # API in future iterations.\n",
    "        os.makedirs(\"model\", exist_ok=True)\n",
    "        torch.save((model.state_dict(), optimizer.state_dict(), lr_scheduler.state_dict()), \"model/checkpoint.pt\")\n",
    "        checkpoint = Checkpoint.from_directory(\"model\")\n",
    "        session.report(metrics=val_metrics.get_metrics(epoch), checkpoint=checkpoint)\n",
    "\n",
    "    train_metrics.close_tensorboard()\n",
    "    val_metrics.close_tensorboard()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a58ec",
   "metadata": {},
   "source": [
    "## Creating the evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d17c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T03:57:06.057174Z",
     "start_time": "2023-03-17T03:57:06.057161Z"
    },
    "code_folding": [
     0,
     38
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def evaluate(test_data, best_result):\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    inpip = InputPipeline(\n",
    "        transformer=best_result.config[\"input_filter\"],\n",
    "        layer_transformer=best_result.config[\"input_layer\"]).to(device)\n",
    "\n",
    "    test_dataset = POCDataset(test_data, transform=inpip, target_transform=None, negative_mining=False)\n",
    "    \n",
    "    if LOAD_DATA_ON_GPU:\n",
    "        evaluation_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "    else:\n",
    "        evaluation_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=20, pin_memory=True, pin_memory_device=device)\n",
    "\n",
    "    best_trained_model = UNet(n_channels=inpip.nb_channel, n_classes=2, bilinear=True, crop=False).to(device)\n",
    "\n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "    model_state, _, _ = torch.load(checkpoint_path)\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "    \n",
    "    loss_fn_name = str(best_result.config[\"loss_combinator\"](best_result.config[\"loss_pixel\"], best_result.config[\"loss_volume\"]))\n",
    "\n",
    "    test_metrics = EvaluationMetrics(\n",
    "        buffer_size=len(evaluation_dataloader),\n",
    "        model_name=best_trained_model.__class__.__name__,\n",
    "        loss_name=loss_fn_name,\n",
    "        opt_name=\"Adam\",\n",
    "        pip_name=str(inpip),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=best_result.config[\"batch_size\"],\n",
    "        learning_rate=best_result.config[\"lr\"],\n",
    "        negative_mining=best_result.config[\"NM\"],\n",
    "        soft_labels=best_result.config[\"SL\"],\n",
    "        device=device)\n",
    "\n",
    "    evaluation_loop(dataloader=evaluation_dataloader, model=best_trained_model, metric=test_metrics, device=device)\n",
    "\n",
    "def evaluate_df(test_data, results_df):\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    results_df.sort_values(\"CrackIoU\", ascending=False, inplace=True)\n",
    "    for index, res in results_df.head(NUM_MODEL_TEST).iterrows():\n",
    "        \n",
    "        inpip = InputPipeline(\n",
    "            transformer=res[\"config/input_filter\"],\n",
    "            layer_transformer=res[\"config/input_layer\"]).to(device)\n",
    "        \n",
    "        test_dataset = POCDataset(test_data, transform=inpip, target_transform=None, negative_mining=False)\n",
    "\n",
    "        if LOAD_DATA_ON_GPU:\n",
    "            evaluation_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "        else:\n",
    "            evaluation_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=20, pin_memory=True, pin_memory_device=device)\n",
    "\n",
    "        trained_model = UNet(n_channels=inpip.nb_channel, n_classes=2, bilinear=True, crop=False).to(device)\n",
    "\n",
    "        checkpoint_path = os.path.join(res[\"logdir\"], \"model/checkpoint.pt\")\n",
    "        model_state, _, _ = torch.load(checkpoint_path)\n",
    "        trained_model.load_state_dict(model_state)\n",
    "        \n",
    "        loss_fn_name = str(res[\"config/loss_combinator\"](res[\"config/loss_pixel\"], res[\"config/loss_volume\"]))\n",
    "\n",
    "        test_metrics = EvaluationMetrics(\n",
    "            buffer_size=len(evaluation_dataloader),\n",
    "            model_name=trained_model.__class__.__name__,\n",
    "            loss_name=loss_fn_name,\n",
    "            opt_name=\"Adam\",\n",
    "            pip_name=str(inpip),\n",
    "            epochs=res[\"Epoch\"],\n",
    "            batch_size=res[\"config/batch_size\"],\n",
    "            learning_rate=res[\"config/lr\"],\n",
    "            negative_mining=res[\"config/NM\"],\n",
    "            soft_labels=res[\"config/SL\"],\n",
    "            device=device)\n",
    "\n",
    "        evaluation_loop(dataloader=evaluation_dataloader, model=trained_model, metric=test_metrics, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e1c03",
   "metadata": {},
   "source": [
    "# Running the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f955b19",
   "metadata": {},
   "source": [
    "## Loading data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a7a2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T02:59:36.980691Z",
     "start_time": "2023-03-17T02:59:02.212581Z"
    }
   },
   "outputs": [],
   "source": [
    "data_reader = POCDataReader(root_dir=\"../data\", load_on_gpu=LOAD_DATA_ON_GPU)\n",
    "train_data, val_data, test_data = data_reader.split([0.7, 0.1, 0.2])\n",
    "data_augment_(train_data, n=NUM_AUGMENT, load_on_gpu=LOAD_DATA_ON_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b0faa",
   "metadata": {},
   "source": [
    "## Configuring the Tuner with a Scheduler and a Search Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69701ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T02:59:48.308691Z",
     "start_time": "2023-03-17T02:59:39.756811Z"
    }
   },
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(max_t=EPOCHS, grace_period=2, reduction_factor=2)\n",
    "search_algo = HyperOptSearch()\n",
    "\n",
    "tune_config = tune.TuneConfig(\n",
    "    metric=\"CrackIoU\",\n",
    "    mode=\"max\",\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    scheduler=scheduler,\n",
    "    search_alg=search_algo)\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        tune.with_parameters(train, train_data=train_data, val_data=val_data),\n",
    "        resources={\"cpu\": CPUS_PER_TRIAL, \"gpu\": GPUS_PER_TRIAL}),\n",
    "    tune_config=tune_config,\n",
    "    param_space=search_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0913de",
   "metadata": {},
   "source": [
    "## Running the Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb3720",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-17T03:56:42.217766Z",
     "start_time": "2023-03-17T02:59:49.996394Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08aabe4",
   "metadata": {},
   "source": [
    "# Evaluating the best Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f9ba97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T05:40:18.913144Z",
     "start_time": "2023-03-10T05:40:18.890272Z"
    }
   },
   "outputs": [],
   "source": [
    "best_result = results.get_best_result(metric=\"CrackIoU\", mode=\"max\", scope=\"all\")  # Get best result object\n",
    "print(\"Best trial config: {}\".format(best_result.config))\n",
    "print(\"Best trial final validation loss: {}\".format(best_result.metrics[\"Loss\"]))\n",
    "print(\"Best trial final validation CrackIoU: {}\".format(best_result.metrics[\"CrackIoU\"]))\n",
    "\n",
    "# evaluate(test_data=test_data, best_result=best_result)\n",
    "\n",
    "results_df = results.get_dataframe(filter_metric=\"CrackIoU\", filter_mode=\"max\")  # Get all trials by CrackIoU\n",
    "results_df.sort_values(\"CrackIoU\", ascending=False, inplace=True)\n",
    "\n",
    "evaluate_df(test_data=test_data, results_df=results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599c84a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "POC-env",
   "language": "python",
   "name": "poc-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
